{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoanAmount            Loan amount in thousands\n",
    "\n",
    "Loan_Amount_Term      Term of loan in months\n",
    "\n",
    "Credit_History        Credit history meets guidelines\n",
    "\n",
    "Property_Area         Urban/ Semi Urban/ Rural\n",
    "\n",
    "Loan_Status           Loan approved (Y/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               object\n",
       "Gender                object\n",
       "Married               object\n",
       "Dependents            object\n",
       "Education             object\n",
       "Self_Employed         object\n",
       "ApplicantIncome        int64\n",
       "CoapplicantIncome    float64\n",
       "LoanAmount           float64\n",
       "Loan_Amount_Term     float64\n",
       "Credit_History       float64\n",
       "Property_Area         object\n",
       "Loan_Status           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for the ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               0\n",
       "Gender               13\n",
       "Married               3\n",
       "Dependents           15\n",
       "Education             0\n",
       "Self_Employed        32\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount           22\n",
       "Loan_Amount_Term     14\n",
       "Credit_History       50\n",
       "Property_Area         0\n",
       "Loan_Status           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dealing with nans\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "list_encoders=[]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "coulmns=None\n",
    "scalar=None\n",
    "means=None\n",
    "def preProcessDataTrain(train):\n",
    "    global means, scalar\n",
    "# High Nans in gender, married, dependents, self_employed, loanAmount, loanTerms, credit_history\n",
    "    NaColsCats=['Gender','Married','Dependents','Self_Employed']\n",
    "#     for x in NaColsCats:\n",
    "#         print(train[x].value_counts(),end='\\n\\n')\n",
    "    train['Gender']=train['Gender'].fillna('Male')\n",
    "    train['Married']=train['Married'].fillna('Yes')\n",
    "    train['Dependents']=train['Dependents'].fillna('0')\n",
    "    train['Self_Employed']=train['Self_Employed'].fillna('No')\n",
    "    train['Credit_History']=train['Credit_History'].fillna('1')\n",
    "#     filling the nas with mean for numeric real values\n",
    "    means=train.mean()\n",
    "    train=train.fillna(train.mean())\n",
    "    global list_encoders\n",
    "    global columns\n",
    "    columns=['Gender','Married','Education','Self_Employed','Property_Area']\n",
    "    for c in columns:\n",
    "        cEncoder=LabelEncoder()\n",
    "        train[c]=cEncoder.fit_transform(train[c].astype('str'))\n",
    "        list_encoders.append(cEncoder)\n",
    "    \n",
    "#     coverting 3+ in train to 3\n",
    "    train['Dependents']=train['Dependents'].replace(['3+'], '3')\n",
    "    train['Loan_Status']=train['Loan_Status'].replace(['Y','N'],[1,0])\n",
    "    # coverting dependents column to integer\n",
    "    train['Dependents']=train['Dependents'].astype(str).astype(int)\n",
    "    \n",
    "#     no one hot encode the columns \n",
    "    col_to_one_hot=['Gender','Married','Education','Self_employed']\n",
    "#     I dont think there is some need to one hot the columns, It would introduce the co linearity in the dataset\n",
    "    \n",
    "    train.drop(['Loan_ID'],axis=1,inplace=True)\n",
    "    scalar = StandardScaler()\n",
    "    train_big_num=train[['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term']]\n",
    "    scalar.fit(train_big_num)\n",
    "#     print(scaler.mean_)\n",
    "\n",
    "    train_np=scalar.transform(train_big_num)\n",
    "    \n",
    "    scaled_features=pd.DataFrame(data=train_np,columns=['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term'])\n",
    "    \n",
    "    sc_feat=['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term']\n",
    "    for sf in sc_feat:\n",
    "        train[sf]=scaled_features[sf]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us one Hot encode some of the columns because numeric assignmnet does not make a sense\n",
    "\n",
    "Columns are Gender, Married, Education, Self_employed\n",
    "\n",
    "\n",
    "\n",
    "I prefer using sklearn.preprocessing.OneHotEncoder instead of pd.get_dummies This is because sklearn.preprocessing.OneHotEncoder returns an object of sklearn.preprocessing.OneHotEncoder class. We can fit this object on the training set and then use the same object to transform the test set. On the other hand, pd.get_dummies returns a dataframe with encodings based on the values in the dataframe we pass to it. This might be good for a quick analysis, but for an extended model building project where you train on training set and will be later testing on a test set, I would suggest using sklearn.preprocessing.OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pp=preProcessDataTrain(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler(copy=True, with_mean=True, with_std=True) \n",
      "\n",
      " ApplicantIncome      5403.459283\n",
      "CoapplicantIncome    1621.245798\n",
      "LoanAmount            146.412162\n",
      "Loan_Amount_Term      342.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(scalar,'\\n\\n',means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes are  ['Female' 'Male']  in  Gender\n",
      "classes are  ['No' 'Yes']  in  Married\n",
      "classes are  ['Graduate' 'Not Graduate']  in  Education\n",
      "classes are  ['No' 'Yes']  in  Self_Employed\n",
      "classes are  ['Rural' 'Semiurban' 'Urban']  in  Property_Area\n"
     ]
    }
   ],
   "source": [
    "# seeing all the lebels\n",
    "for x,c in zip(list_encoders,columns):\n",
    "    print('classes are ',x.classes_,' in ',c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "  Credit_History Property_Area Loan_Status  \n",
       "0              1         Urban           Y  \n",
       "1              1         Rural           N  \n",
       "2              1         Urban           Y  \n",
       "3              1         Urban           Y  \n",
       "4              1         Urban           Y  "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.072991</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.134412</td>\n",
       "      <td>-0.038732</td>\n",
       "      <td>-0.219273</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.393747</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.957641</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.462062</td>\n",
       "      <td>0.251980</td>\n",
       "      <td>-0.314547</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097728</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.064454</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.880600</td>\n",
       "      <td>1.436099</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.503019</td>\n",
       "      <td>-0.035995</td>\n",
       "      <td>-0.612275</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.387850</td>\n",
       "      <td>0.301914</td>\n",
       "      <td>0.138001</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.228939</td>\n",
       "      <td>-0.032575</td>\n",
       "      <td>0.257093</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.218457</td>\n",
       "      <td>3.196713</td>\n",
       "      <td>2.412650</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.360982</td>\n",
       "      <td>-0.315078</td>\n",
       "      <td>-0.910004</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.475660</td>\n",
       "      <td>0.074817</td>\n",
       "      <td>-0.445547</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.381788</td>\n",
       "      <td>2.217871</td>\n",
       "      <td>0.638186</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.581655</td>\n",
       "      <td>0.416830</td>\n",
       "      <td>-0.386002</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.672414</td>\n",
       "      <td>-0.183061</td>\n",
       "      <td>-1.541190</td>\n",
       "      <td>-3.451490</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.074288</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.255001</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.296107</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.552730</td>\n",
       "      <td>-1.585820</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.310196</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.838549</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.084609</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.159728</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.459277</td>\n",
       "      <td>0.642559</td>\n",
       "      <td>-0.374092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.369678</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.505093</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.090356</td>\n",
       "      <td>1.369336</td>\n",
       "      <td>2.007739</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.459277</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>-0.362183</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.333951</td>\n",
       "      <td>0.101152</td>\n",
       "      <td>-0.409820</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.276285</td>\n",
       "      <td>0.445901</td>\n",
       "      <td>0.054637</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680946</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>0.531004</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.426676</td>\n",
       "      <td>0.216068</td>\n",
       "      <td>-0.290728</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.192898</td>\n",
       "      <td>-0.198794</td>\n",
       "      <td>-0.433638</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.648987</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-1.326825</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.270878</td>\n",
       "      <td>0.157926</td>\n",
       "      <td>-0.314547</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.363931</td>\n",
       "      <td>0.443165</td>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.186194</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>0.697732</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.584440</td>\n",
       "      <td>11.018211</td>\n",
       "      <td>-0.671821</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.366552</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-1.314916</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.577232</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-1.017187</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.428642</td>\n",
       "      <td>0.101152</td>\n",
       "      <td>-0.004909</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.183560</td>\n",
       "      <td>0.471552</td>\n",
       "      <td>0.304730</td>\n",
       "      <td>-4.011191</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.508916</td>\n",
       "      <td>-0.033943</td>\n",
       "      <td>-0.505093</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.530541</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.910004</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.107053</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.624185</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.438635</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.481275</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.393747</td>\n",
       "      <td>0.613829</td>\n",
       "      <td>-1.076733</td>\n",
       "      <td>-2.518655</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097728</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>0.697732</td>\n",
       "      <td>-1.585820</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.647690</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>1.733828</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.253021</td>\n",
       "      <td>0.574156</td>\n",
       "      <td>-0.052545</td>\n",
       "      <td>-2.518655</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.755640</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>1.352735</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.257281</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.433638</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.160474</td>\n",
       "      <td>-0.212474</td>\n",
       "      <td>0.483367</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.395877</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.695640</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.746968</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>0.400003</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061687</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>0.542913</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.817072</td>\n",
       "      <td>13.696173</td>\n",
       "      <td>2.424559</td>\n",
       "      <td>-2.518655</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.411113</td>\n",
       "      <td>0.400413</td>\n",
       "      <td>0.102274</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049072</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.219273</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.283001</td>\n",
       "      <td>0.916511</td>\n",
       "      <td>0.304730</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.080680</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>4.163296</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.492042</td>\n",
       "      <td>0.745163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.518655</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.328217</td>\n",
       "      <td>0.300545</td>\n",
       "      <td>0.316639</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.232052</td>\n",
       "      <td>-0.071907</td>\n",
       "      <td>0.126092</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.355740</td>\n",
       "      <td>0.112438</td>\n",
       "      <td>-0.457456</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>609 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gender  Married  Dependents  Education  Self_Employed  ApplicantIncome  \\\n",
       "0         1        0           0          0              0         0.072991   \n",
       "1         1        1           1          0              0        -0.134412   \n",
       "2         1        1           0          0              1        -0.393747   \n",
       "3         1        1           0          1              0        -0.462062   \n",
       "4         1        0           0          0              0         0.097728   \n",
       "5         1        1           2          0              1         0.002218   \n",
       "6         1        1           0          1              0        -0.503019   \n",
       "7         1        1           3          0              0        -0.387850   \n",
       "8         1        1           2          0              0        -0.228939   \n",
       "9         1        1           1          0              0         1.218457   \n",
       "10        1        1           2          0              0        -0.360982   \n",
       "11        1        1           2          0              0        -0.475660   \n",
       "12        1        1           2          0              0        -0.381788   \n",
       "13        1        0           0          0              0        -0.581655   \n",
       "14        1        1           2          0              0        -0.672414   \n",
       "15        1        0           0          0              0        -0.074288   \n",
       "16        1        0           1          1              0        -0.296107   \n",
       "17        0        0           0          0              0        -0.310196   \n",
       "18        1        1           0          1              0        -0.084609   \n",
       "19        1        1           0          0              0        -0.459277   \n",
       "20        1        1           0          1              0         0.369678   \n",
       "21        1        1           1          0              0         0.090356   \n",
       "22        1        1           0          1              0        -0.459277   \n",
       "23        1        1           2          1              0        -0.333951   \n",
       "24        1        1           1          0              0        -0.276285   \n",
       "25        1        1           0          0              1         0.680946   \n",
       "26        1        1           0          0              0        -0.426676   \n",
       "27        1        1           2          1              0        -0.192898   \n",
       "28        1        0           0          1              0        -0.648987   \n",
       "29        0        0           2          0              0        -0.270878   \n",
       "..      ...      ...         ...        ...            ...              ...   \n",
       "579       1        0           0          0              0        -0.363931   \n",
       "580       1        1           2          0              0         0.186194   \n",
       "581       1        0           0          0              0        -0.584440   \n",
       "582       0        1           0          0              0        -0.366552   \n",
       "583       1        1           1          0              0        -0.577232   \n",
       "584       1        1           1          0              0        -0.428642   \n",
       "585       1        1           1          0              0        -0.183560   \n",
       "586       1        1           0          0              0        -0.508916   \n",
       "587       0        0           0          1              0        -0.530541   \n",
       "588       1        0           0          0              0        -0.107053   \n",
       "589       1        1           2          0              1        -0.438635   \n",
       "590       1        1           0          0              0        -0.393747   \n",
       "591       1        1           2          0              1         0.097728   \n",
       "592       1        0           3          0              1         0.647690   \n",
       "593       1        1           0          0              0        -0.253021   \n",
       "594       1        1           0          0              1         1.755640   \n",
       "595       1        0           0          1              0        -0.257281   \n",
       "596       1        1           2          1              1         0.160474   \n",
       "597       1        0           0          0              0        -0.395877   \n",
       "598       1        1           0          0              1         0.746968   \n",
       "599       1        1           2          0              0         0.061687   \n",
       "600       0        0           3          0              0        -0.817072   \n",
       "601       1        1           0          1              0        -0.411113   \n",
       "602       1        1           3          0              0         0.049072   \n",
       "603       1        0           0          0              0        -0.283001   \n",
       "604       0        1           1          0              0         1.080680   \n",
       "605       1        1           0          1              0        -0.492042   \n",
       "606       1        1           1          0              0        -0.328217   \n",
       "607       1        1           2          1              0        -0.232052   \n",
       "608       1        1           0          0              0        -0.355740   \n",
       "\n",
       "     CoapplicantIncome  LoanAmount  Loan_Amount_Term Credit_History  \\\n",
       "0            -0.554487    0.000000          0.279851              1   \n",
       "1            -0.038732   -0.219273          0.279851              1   \n",
       "2            -0.554487   -0.957641          0.279851              1   \n",
       "3             0.251980   -0.314547          0.279851              1   \n",
       "4            -0.554487   -0.064454          0.279851              1   \n",
       "5             0.880600    1.436099          0.279851              1   \n",
       "6            -0.035995   -0.612275          0.279851              1   \n",
       "7             0.301914    0.138001          0.279851              0   \n",
       "8            -0.032575    0.257093          0.279851              1   \n",
       "9             3.196713    2.412650          0.279851              1   \n",
       "10           -0.315078   -0.910004          0.279851              1   \n",
       "11            0.074817   -0.445547          0.279851              1   \n",
       "12            2.217871    0.638186          0.279851              1   \n",
       "13            0.416830   -0.386002          0.279851              1   \n",
       "14           -0.183061   -1.541190         -3.451490              1   \n",
       "15           -0.554487   -0.255001          0.279851              1   \n",
       "16           -0.554487   -0.552730         -1.585820              1   \n",
       "17           -0.554487   -0.838549          0.279851              0   \n",
       "18           -0.554487   -0.159728          0.279851              1   \n",
       "19            0.642559   -0.374092          0.000000              1   \n",
       "20           -0.554487   -0.505093          0.279851              0   \n",
       "21            1.369336    2.007739          0.279851              1   \n",
       "22            0.099100   -0.362183          0.279851              0   \n",
       "23            0.101152   -0.409820          0.279851              0   \n",
       "24            0.445901    0.054637          0.279851              1   \n",
       "25           -0.554487    0.531004          0.279851              1   \n",
       "26            0.216068   -0.290728          0.279851              1   \n",
       "27           -0.198794   -0.433638          0.279851              1   \n",
       "28           -0.554487   -1.326825          0.279851              1   \n",
       "29            0.157926   -0.314547          0.279851              1   \n",
       "..                 ...         ...               ...            ...   \n",
       "579           0.443165    0.173729          0.279851              1   \n",
       "580          -0.554487    0.697732          0.279851              1   \n",
       "581          11.018211   -0.671821          0.279851              1   \n",
       "582          -0.554487   -1.314916          0.279851              1   \n",
       "583          -0.554487   -1.017187          0.279851              1   \n",
       "584           0.101152   -0.004909          0.279851              0   \n",
       "585           0.471552    0.304730         -4.011191              1   \n",
       "586          -0.033943   -0.505093          0.279851              1   \n",
       "587          -0.554487   -0.910004          0.279851              1   \n",
       "588          -0.554487   -0.624185          0.279851              1   \n",
       "589          -0.554487   -0.481275          0.279851              0   \n",
       "590           0.613829   -1.076733         -2.518655              1   \n",
       "591          -0.554487    0.697732         -1.585820              1   \n",
       "592          -0.554487    1.733828          0.279851              1   \n",
       "593           0.574156   -0.052545         -2.518655              1   \n",
       "594          -0.554487    1.352735          0.279851              1   \n",
       "595          -0.554487   -0.433638          0.279851              1   \n",
       "596          -0.212474    0.483367          0.279851              1   \n",
       "597          -0.554487   -0.695640          0.279851              0   \n",
       "598          -0.554487    0.400003          0.279851              1   \n",
       "599          -0.554487    0.542913          0.279851              1   \n",
       "600          13.696173    2.424559         -2.518655              1   \n",
       "601           0.400413    0.102274          0.279851              1   \n",
       "602          -0.554487   -0.219273          0.279851              1   \n",
       "603           0.916511    0.304730          0.279851              1   \n",
       "604          -0.554487    4.163296          0.279851              1   \n",
       "605           0.745163    0.000000         -2.518655              1   \n",
       "606           0.300545    0.316639          0.279851              1   \n",
       "607          -0.071907    0.126092          0.279851              1   \n",
       "608           0.112438   -0.457456          0.279851              1   \n",
       "\n",
       "     Property_Area  Loan_Status  \n",
       "0                2            1  \n",
       "1                0            0  \n",
       "2                2            1  \n",
       "3                2            1  \n",
       "4                2            1  \n",
       "5                2            1  \n",
       "6                2            1  \n",
       "7                1            0  \n",
       "8                2            1  \n",
       "9                1            0  \n",
       "10               2            1  \n",
       "11               2            1  \n",
       "12               2            1  \n",
       "13               0            0  \n",
       "14               2            1  \n",
       "15               2            1  \n",
       "16               2            1  \n",
       "17               2            0  \n",
       "18               0            0  \n",
       "19               2            1  \n",
       "20               2            0  \n",
       "21               2            1  \n",
       "22               1            0  \n",
       "23               0            0  \n",
       "24               1            0  \n",
       "25               1            1  \n",
       "26               1            1  \n",
       "27               2            1  \n",
       "28               2            0  \n",
       "29               1            1  \n",
       "..             ...          ...  \n",
       "579              2            1  \n",
       "580              1            1  \n",
       "581              2            0  \n",
       "582              1            1  \n",
       "583              0            0  \n",
       "584              0            0  \n",
       "585              0            0  \n",
       "586              2            1  \n",
       "587              1            1  \n",
       "588              1            1  \n",
       "589              1            0  \n",
       "590              1            1  \n",
       "591              1            0  \n",
       "592              1            1  \n",
       "593              0            1  \n",
       "594              2            1  \n",
       "595              0            1  \n",
       "596              0            0  \n",
       "597              1            0  \n",
       "598              0            1  \n",
       "599              2            1  \n",
       "600              2            0  \n",
       "601              0            1  \n",
       "602              2            1  \n",
       "603              0            1  \n",
       "604              1            1  \n",
       "605              2            0  \n",
       "606              1            1  \n",
       "607              0            1  \n",
       "608              0            1  \n",
       "\n",
       "[609 rows x 12 columns]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pp.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_pp.drop(['Loan_Status'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=train_pp[['Loan_Status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1\n",
    "\n",
    "Making Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc is  0.8150851581508516\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='warn', n_jobs=1, penalty='l1', random_state=0,\n",
      "                   solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "test acc is  0.7980295566502463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf_lr = LogisticRegression(penalty='l1',random_state=0,solver='liblinear',max_iter=1000,n_jobs=1)\n",
    "clf_lr=clf_lr.fit(x_train, np.squeeze(y_train.values))\n",
    "train_preds=clf_lr.predict(x_train)\n",
    "train_acc=accuracy_score(y_train, train_preds)\n",
    "print('train acc is ',train_acc)\n",
    "\n",
    "print(clf_lr)\n",
    "\n",
    "test_preds=clf_lr.predict(x_test)\n",
    "test_acc=accuracy_score(y_test,test_preds)\n",
    "print('test acc is ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('lr.txt',train_preds,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l1 and l2 both are giving same 81% accuracy, not much accuracy but very less overfitting\n",
    "\n",
    "##### Model 2\n",
    "Let us try SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][[ 4.94995433e-09 -1.80712115e-09  9.78032843e-10  4.30459313e-09\n",
      "   3.13153350e-09 -2.45242830e-09 -1.07197341e-09  1.24801998e-09\n",
      "   3.07897825e-09  2.00000001e+00  1.09279973e-09]]\n",
      "[-1.00000002]\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
      "          penalty='l2', random_state=0, tol=1e-05, verbose=1)\n",
      "train acc is  0.8150851581508516\n",
      "test acc is  0.7980295566502463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf_svm = LinearSVC(tol=1e-5,penalty='l2',loss='hinge',dual=True,C=1.0,verbose=1,random_state=0,max_iter=1000)\n",
    "# C,max_iter to be tuned\n",
    "\n",
    "clf_svm=clf_svm.fit(x_train, np.squeeze(y_train.values))  \n",
    "print(clf_svm.coef_)\n",
    "print(clf_svm.intercept_)\n",
    "\n",
    "print(clf_svm)\n",
    "\n",
    "train_preds=clf_svm.predict(x_train)\n",
    "train_acc=accuracy_score(y_train, train_preds)\n",
    "print('train acc is ',train_acc)\n",
    "\n",
    "\n",
    "test_preds=clf_svm.predict(x_test)\n",
    "test_acc=accuracy_score(y_test,test_preds)\n",
    "print('test acc is ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('svm.txt',train_preds,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not any good results from SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3 Naive Bayes\n",
    "\n",
    "Provide a good base Model in classification, let us see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<bound method BaseEstimator.get_params of GaussianNB(priors=None, var_smoothing=1e-09)>\n",
      "train acc is  0.8150851581508516\n",
      "test acc is  0.7980295566502463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_nb = GaussianNB()\n",
    "print(clf_nb)\n",
    "clf_nb=clf_nb.fit(x_train, np.squeeze(y_train.values))  \n",
    "\n",
    "print(clf_nb)\n",
    "\n",
    "print(clf_nb.get_params)\n",
    "\n",
    "train_preds=clf_nb.predict(x_train)\n",
    "train_acc=accuracy_score(y_train, train_preds)\n",
    "print('train acc is ',train_acc)\n",
    "\n",
    "\n",
    "test_preds=clf_nb.predict(x_test)\n",
    "test_acc=accuracy_score(y_test,test_preds)\n",
    "print('test acc is ',test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('nb.txt',train_preds,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 4 Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02264591 0.01286038 0.0470388  0.00588488 0.04621277 0.27762548\n",
      " 0.09105327 0.12444637 0.03731642 0.27714152 0.0577742 ]\n",
      "\n",
      "\n",
      "train acc is  1.0\n",
      "test acc is  0.6798029556650246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "# print(clf)\n",
    "clf=clf.fit(x_train,np.squeeze(y_train.values))\n",
    "print(clf.feature_importances_,end='\\n\\n\\n')\n",
    "train_preds=clf.predict(x_train)\n",
    "train_acc=accuracy_score(y_train, train_preds)\n",
    "print('train acc is ',train_acc)\n",
    "\n",
    "\n",
    "test_preds=clf.predict(x_test)\n",
    "test_acc=accuracy_score(y_test,test_preds)\n",
    "print('test acc is ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27, 15 and 28% are top 3 feature contribution, and lots of overfitting let us do some regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_.sum()\n",
    "# and they sum to 99.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411, 11)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(101.50285714285715, 211.7178947368421, 'X[9] <= 0.5\\nentropy = 0.413\\nsamples = 411\\nvalue = [120, 291]'),\n",
       " Text(56.331428571428575, 200.2736842105263, 'X[6] <= 2.221\\nentropy = 0.168\\nsamples = 54\\nvalue = [49, 5]'),\n",
       " Text(52.080000000000005, 188.82947368421054, 'X[5] <= -0.178\\nentropy = 0.14\\nsamples = 53\\nvalue = [49, 4]'),\n",
       " Text(43.57714285714286, 177.38526315789474, 'X[8] <= -1.586\\nentropy = 0.057\\nsamples = 34\\nvalue = [33, 1]'),\n",
       " Text(39.32571428571429, 165.94105263157894, 'X[5] <= -0.468\\nentropy = 0.245\\nsamples = 7\\nvalue = [6, 1]'),\n",
       " Text(35.074285714285715, 154.49684210526317, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(43.57714285714286, 154.49684210526317, 'entropy = 0.0\\nsamples = 6\\nvalue = [6, 0]'),\n",
       " Text(47.82857142857143, 165.94105263157894, 'entropy = 0.0\\nsamples = 27\\nvalue = [27, 0]'),\n",
       " Text(60.58285714285715, 177.38526315789474, 'X[5] <= -0.063\\nentropy = 0.266\\nsamples = 19\\nvalue = [16, 3]'),\n",
       " Text(56.331428571428575, 165.94105263157894, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(64.83428571428571, 165.94105263157894, 'entropy = 0.0\\nsamples = 16\\nvalue = [16, 0]'),\n",
       " Text(60.58285714285715, 188.82947368421054, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(146.67428571428573, 200.2736842105263, 'X[1] <= 0.5\\nentropy = 0.319\\nsamples = 357\\nvalue = [71, 286]'),\n",
       " Text(89.28, 188.82947368421054, 'X[8] <= -4.571\\nentropy = 0.395\\nsamples = 133\\nvalue = [36, 97]'),\n",
       " Text(85.02857142857144, 177.38526315789474, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(93.53142857142858, 177.38526315789474, 'X[5] <= 0.102\\nentropy = 0.384\\nsamples = 131\\nvalue = [34, 97]'),\n",
       " Text(73.33714285714287, 165.94105263157894, 'X[8] <= 1.213\\nentropy = 0.344\\nsamples = 104\\nvalue = [23, 81]'),\n",
       " Text(62.70857142857143, 154.49684210526317, 'X[5] <= -0.577\\nentropy = 0.32\\nsamples = 100\\nvalue = [20, 80]'),\n",
       " Text(54.20571428571429, 143.05263157894737, 'X[6] <= 0.216\\nentropy = 0.48\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(49.95428571428572, 131.60842105263157, 'X[7] <= -1.255\\nentropy = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(45.70285714285715, 120.16421052631578, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(54.20571428571429, 120.16421052631578, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(58.45714285714286, 131.60842105263157, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(71.21142857142858, 143.05263157894737, 'X[6] <= 0.173\\nentropy = 0.294\\nsamples = 95\\nvalue = [17, 78]'),\n",
       " Text(66.96000000000001, 131.60842105263157, 'X[5] <= -0.109\\nentropy = 0.347\\nsamples = 76\\nvalue = [17, 59]'),\n",
       " Text(62.70857142857143, 120.16421052631578, 'X[5] <= -0.129\\nentropy = 0.386\\nsamples = 65\\nvalue = [17, 48]'),\n",
       " Text(51.01714285714286, 108.72, 'X[4] <= 0.5\\nentropy = 0.339\\nsamples = 60\\nvalue = [13, 47]'),\n",
       " Text(40.38857142857143, 97.27578947368421, 'X[5] <= -0.294\\nentropy = 0.311\\nsamples = 57\\nvalue = [11, 46]'),\n",
       " Text(31.88571428571429, 85.83157894736843, 'X[5] <= -0.296\\nentropy = 0.401\\nsamples = 36\\nvalue = [10, 26]'),\n",
       " Text(27.634285714285717, 74.38736842105263, 'X[7] <= -0.136\\nentropy = 0.36\\nsamples = 34\\nvalue = [8, 26]'),\n",
       " Text(17.005714285714287, 62.943157894736856, 'X[10] <= 0.5\\nentropy = 0.285\\nsamples = 29\\nvalue = [5, 24]'),\n",
       " Text(8.502857142857144, 51.49894736842106, 'X[5] <= -0.509\\nentropy = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(4.251428571428572, 40.05473684210526, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(12.754285714285714, 40.05473684210526, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(25.50857142857143, 51.49894736842106, 'X[0] <= 0.5\\nentropy = 0.204\\nsamples = 26\\nvalue = [3, 23]'),\n",
       " Text(21.25714285714286, 40.05473684210526, 'entropy = 0.0\\nsamples = 13\\nvalue = [0, 13]'),\n",
       " Text(29.76, 40.05473684210526, 'X[7] <= -0.88\\nentropy = 0.355\\nsamples = 13\\nvalue = [3, 10]'),\n",
       " Text(25.50857142857143, 28.610526315789485, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 6]'),\n",
       " Text(34.011428571428574, 28.610526315789485, 'X[10] <= 1.5\\nentropy = 0.49\\nsamples = 7\\nvalue = [3, 4]'),\n",
       " Text(29.76, 17.166315789473686, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(38.26285714285714, 17.166315789473686, 'X[2] <= 0.5\\nentropy = 0.375\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(34.011428571428574, 5.722105263157886, 'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(42.51428571428572, 5.722105263157886, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(38.26285714285714, 62.943157894736856, 'X[10] <= 1.0\\nentropy = 0.48\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(34.011428571428574, 51.49894736842106, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(42.51428571428572, 51.49894736842106, 'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(36.13714285714286, 74.38736842105263, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(48.89142857142858, 85.83157894736843, 'X[5] <= -0.195\\nentropy = 0.091\\nsamples = 21\\nvalue = [1, 20]'),\n",
       " Text(44.64, 74.38736842105263, 'entropy = 0.0\\nsamples = 15\\nvalue = [0, 15]'),\n",
       " Text(53.142857142857146, 74.38736842105263, 'X[5] <= -0.182\\nentropy = 0.278\\nsamples = 6\\nvalue = [1, 5]'),\n",
       " Text(48.89142857142858, 62.943157894736856, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(57.39428571428572, 62.943157894736856, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 5]'),\n",
       " Text(61.64571428571429, 97.27578947368421, 'X[10] <= 1.5\\nentropy = 0.444\\nsamples = 3\\nvalue = [2, 1]'),\n",
       " Text(57.39428571428572, 85.83157894736843, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(65.89714285714287, 85.83157894736843, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(74.4, 108.72, 'X[2] <= 2.0\\nentropy = 0.32\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(70.14857142857143, 97.27578947368421, 'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(78.65142857142858, 97.27578947368421, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(71.21142857142858, 120.16421052631578, 'entropy = 0.0\\nsamples = 11\\nvalue = [0, 11]'),\n",
       " Text(75.46285714285715, 131.60842105263157, 'entropy = 0.0\\nsamples = 19\\nvalue = [0, 19]'),\n",
       " Text(83.9657142857143, 154.49684210526317, 'X[5] <= -0.628\\nentropy = 0.375\\nsamples = 4\\nvalue = [3, 1]'),\n",
       " Text(79.71428571428572, 143.05263157894737, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(88.21714285714286, 143.05263157894737, 'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(113.72571428571429, 165.94105263157894, 'X[4] <= 0.5\\nentropy = 0.483\\nsamples = 27\\nvalue = [11, 16]'),\n",
       " Text(100.97142857142858, 154.49684210526317, 'X[0] <= 0.5\\nentropy = 0.459\\nsamples = 14\\nvalue = [9, 5]'),\n",
       " Text(96.72000000000001, 143.05263157894737, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(105.22285714285715, 143.05263157894737, 'X[5] <= 0.616\\nentropy = 0.298\\nsamples = 11\\nvalue = [9, 2]'),\n",
       " Text(100.97142857142858, 131.60842105263157, 'entropy = 0.0\\nsamples = 5\\nvalue = [5, 0]'),\n",
       " Text(109.47428571428573, 131.60842105263157, 'X[5] <= 0.787\\nentropy = 0.444\\nsamples = 6\\nvalue = [4, 2]'),\n",
       " Text(105.22285714285715, 120.16421052631578, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(113.72571428571429, 120.16421052631578, 'X[7] <= 2.573\\nentropy = 0.32\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(109.47428571428573, 108.72, 'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(117.97714285714287, 108.72, 'X[7] <= 3.526\\nentropy = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(113.72571428571429, 97.27578947368421, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(122.22857142857144, 97.27578947368421, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(126.48, 154.49684210526317, 'X[5] <= 0.135\\nentropy = 0.26\\nsamples = 13\\nvalue = [2, 11]'),\n",
       " Text(122.22857142857144, 143.05263157894737, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(130.73142857142858, 143.05263157894737, 'X[10] <= 0.5\\nentropy = 0.153\\nsamples = 12\\nvalue = [1, 11]'),\n",
       " Text(126.48, 131.60842105263157, 'X[5] <= 0.283\\nentropy = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(122.22857142857144, 120.16421052631578, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(130.73142857142858, 120.16421052631578, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(134.98285714285714, 131.60842105263157, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 9]'),\n",
       " Text(204.06857142857143, 188.82947368421054, 'X[10] <= 0.5\\nentropy = 0.264\\nsamples = 224\\nvalue = [35, 189]'),\n",
       " Text(148.8, 177.38526315789474, 'X[7] <= -0.678\\nentropy = 0.387\\nsamples = 61\\nvalue = [16, 45]'),\n",
       " Text(140.29714285714286, 165.94105263157894, 'X[5] <= -0.451\\nentropy = 0.32\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(136.0457142857143, 154.49684210526317, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(144.54857142857145, 154.49684210526317, 'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(157.30285714285716, 165.94105263157894, 'X[5] <= -0.789\\nentropy = 0.337\\nsamples = 56\\nvalue = [12, 44]'),\n",
       " Text(153.05142857142857, 154.49684210526317, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(161.55428571428573, 154.49684210526317, 'X[2] <= 2.5\\nentropy = 0.32\\nsamples = 55\\nvalue = [11, 44]'),\n",
       " Text(157.30285714285716, 143.05263157894737, 'X[5] <= -0.354\\nentropy = 0.369\\nsamples = 45\\nvalue = [11, 34]'),\n",
       " Text(143.4857142857143, 131.60842105263157, 'X[6] <= 0.094\\nentropy = 0.153\\nsamples = 12\\nvalue = [1, 11]'),\n",
       " Text(139.23428571428573, 120.16421052631578, 'X[6] <= 0.063\\nentropy = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(134.98285714285714, 108.72, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(143.4857142857143, 108.72, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(147.73714285714286, 120.16421052631578, 'entropy = 0.0\\nsamples = 10\\nvalue = [0, 10]'),\n",
       " Text(171.12, 131.60842105263157, 'X[5] <= -0.08\\nentropy = 0.422\\nsamples = 33\\nvalue = [10, 23]'),\n",
       " Text(160.49142857142857, 120.16421052631578, 'X[2] <= 0.5\\nentropy = 0.497\\nsamples = 13\\nvalue = [7, 6]'),\n",
       " Text(151.98857142857145, 108.72, 'X[5] <= -0.323\\nentropy = 0.469\\nsamples = 8\\nvalue = [3, 5]'),\n",
       " Text(147.73714285714286, 97.27578947368421, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(156.24, 97.27578947368421, 'X[7] <= -0.386\\nentropy = 0.278\\nsamples = 6\\nvalue = [1, 5]'),\n",
       " Text(151.98857142857145, 85.83157894736843, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(160.49142857142857, 85.83157894736843, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 5]'),\n",
       " Text(168.99428571428572, 108.72, 'X[4] <= 0.5\\nentropy = 0.32\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(164.74285714285716, 97.27578947368421, 'entropy = 0.0\\nsamples = 4\\nvalue = [4, 0]'),\n",
       " Text(173.2457142857143, 97.27578947368421, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(181.74857142857144, 120.16421052631578, 'X[7] <= 0.412\\nentropy = 0.255\\nsamples = 20\\nvalue = [3, 17]'),\n",
       " Text(177.49714285714288, 108.72, 'entropy = 0.0\\nsamples = 11\\nvalue = [0, 11]'),\n",
       " Text(186.00000000000003, 108.72, 'X[8] <= -0.187\\nentropy = 0.444\\nsamples = 9\\nvalue = [3, 6]'),\n",
       " Text(181.74857142857144, 97.27578947368421, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(190.2514285714286, 97.27578947368421, 'X[6] <= -0.383\\nentropy = 0.5\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(186.00000000000003, 85.83157894736843, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(194.50285714285715, 85.83157894736843, 'X[7] <= 0.626\\nentropy = 0.48\\nsamples = 5\\nvalue = [3, 2]'),\n",
       " Text(190.2514285714286, 74.38736842105263, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(198.75428571428574, 74.38736842105263, 'X[5] <= 0.969\\nentropy = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(194.50285714285715, 62.943157894736856, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(203.0057142857143, 62.943157894736856, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(165.8057142857143, 143.05263157894737, 'entropy = 0.0\\nsamples = 10\\nvalue = [0, 10]'),\n",
       " Text(259.33714285714285, 177.38526315789474, 'X[6] <= 4.252\\nentropy = 0.206\\nsamples = 163\\nvalue = [19, 144]'),\n",
       " Text(255.08571428571432, 165.94105263157894, 'X[6] <= -0.522\\nentropy = 0.198\\nsamples = 162\\nvalue = [18, 144]'),\n",
       " Text(223.20000000000002, 154.49684210526317, 'X[7] <= -0.154\\nentropy = 0.316\\nsamples = 56\\nvalue = [11, 45]'),\n",
       " Text(203.0057142857143, 143.05263157894737, 'X[7] <= -0.928\\nentropy = 0.147\\nsamples = 25\\nvalue = [2, 23]'),\n",
       " Text(194.50285714285715, 131.60842105263157, 'X[7] <= -1.196\\nentropy = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(190.2514285714286, 120.16421052631578, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(198.75428571428574, 120.16421052631578, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(211.50857142857146, 131.60842105263157, 'X[10] <= 1.5\\nentropy = 0.083\\nsamples = 23\\nvalue = [1, 22]'),\n",
       " Text(207.25714285714287, 120.16421052631578, 'entropy = 0.0\\nsamples = 14\\nvalue = [0, 14]'),\n",
       " Text(215.76000000000002, 120.16421052631578, 'X[5] <= -0.127\\nentropy = 0.198\\nsamples = 9\\nvalue = [1, 8]'),\n",
       " Text(211.50857142857146, 108.72, 'X[5] <= -0.161\\nentropy = 0.444\\nsamples = 3\\nvalue = [1, 2]'),\n",
       " Text(207.25714285714287, 97.27578947368421, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(215.76000000000002, 97.27578947368421, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(220.01142857142858, 108.72, 'entropy = 0.0\\nsamples = 6\\nvalue = [0, 6]'),\n",
       " Text(243.39428571428573, 143.05263157894737, 'X[5] <= -0.125\\nentropy = 0.412\\nsamples = 31\\nvalue = [9, 22]'),\n",
       " Text(239.14285714285717, 131.60842105263157, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(247.64571428571432, 131.60842105263157, 'X[5] <= 0.579\\nentropy = 0.366\\nsamples = 29\\nvalue = [7, 22]'),\n",
       " Text(237.0171428571429, 120.16421052631578, 'X[2] <= 0.5\\nentropy = 0.473\\nsamples = 13\\nvalue = [5, 8]'),\n",
       " Text(228.51428571428573, 108.72, 'X[7] <= 0.067\\nentropy = 0.32\\nsamples = 5\\nvalue = [4, 1]'),\n",
       " Text(224.26285714285717, 97.27578947368421, 'X[3] <= 0.5\\nentropy = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(220.01142857142858, 85.83157894736843, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(228.51428571428573, 85.83157894736843, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(232.7657142857143, 97.27578947368421, 'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(245.52, 108.72, 'X[4] <= 0.5\\nentropy = 0.219\\nsamples = 8\\nvalue = [1, 7]'),\n",
       " Text(241.26857142857145, 97.27578947368421, 'entropy = 0.0\\nsamples = 7\\nvalue = [0, 7]'),\n",
       " Text(249.7714285714286, 97.27578947368421, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(258.2742857142857, 120.16421052631578, 'X[5] <= 1.982\\nentropy = 0.219\\nsamples = 16\\nvalue = [2, 14]'),\n",
       " Text(254.02285714285716, 108.72, 'entropy = 0.0\\nsamples = 12\\nvalue = [0, 12]'),\n",
       " Text(262.5257142857143, 108.72, 'X[7] <= 4.806\\nentropy = 0.5\\nsamples = 4\\nvalue = [2, 2]'),\n",
       " Text(258.2742857142857, 97.27578947368421, 'entropy = 0.0\\nsamples = 2\\nvalue = [2, 0]'),\n",
       " Text(266.7771428571429, 97.27578947368421, 'entropy = 0.0\\nsamples = 2\\nvalue = [0, 2]'),\n",
       " Text(286.9714285714286, 154.49684210526317, 'X[8] <= -1.586\\nentropy = 0.123\\nsamples = 106\\nvalue = [7, 99]'),\n",
       " Text(275.28000000000003, 143.05263157894737, 'X[5] <= -0.462\\nentropy = 0.32\\nsamples = 15\\nvalue = [3, 12]'),\n",
       " Text(271.02857142857147, 131.60842105263157, 'X[6] <= 0.063\\nentropy = 0.5\\nsamples = 6\\nvalue = [3, 3]'),\n",
       " Text(266.7771428571429, 120.16421052631578, 'entropy = 0.0\\nsamples = 3\\nvalue = [0, 3]'),\n",
       " Text(275.28000000000003, 120.16421052631578, 'entropy = 0.0\\nsamples = 3\\nvalue = [3, 0]'),\n",
       " Text(279.5314285714286, 131.60842105263157, 'entropy = 0.0\\nsamples = 9\\nvalue = [0, 9]'),\n",
       " Text(298.66285714285715, 143.05263157894737, 'X[6] <= 0.278\\nentropy = 0.084\\nsamples = 91\\nvalue = [4, 87]'),\n",
       " Text(288.0342857142857, 131.60842105263157, 'X[7] <= -0.014\\nentropy = 0.032\\nsamples = 61\\nvalue = [1, 60]'),\n",
       " Text(283.78285714285715, 120.16421052631578, 'entropy = 0.0\\nsamples = 44\\nvalue = [0, 44]'),\n",
       " Text(292.28571428571433, 120.16421052631578, 'X[7] <= 0.009\\nentropy = 0.111\\nsamples = 17\\nvalue = [1, 16]'),\n",
       " Text(288.0342857142857, 108.72, 'X[5] <= -0.457\\nentropy = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(283.78285714285715, 97.27578947368421, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(292.28571428571433, 97.27578947368421, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(296.5371428571429, 108.72, 'entropy = 0.0\\nsamples = 15\\nvalue = [0, 15]'),\n",
       " Text(309.2914285714286, 131.60842105263157, 'X[6] <= 0.285\\nentropy = 0.18\\nsamples = 30\\nvalue = [3, 27]'),\n",
       " Text(305.04, 120.16421052631578, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(313.54285714285714, 120.16421052631578, 'X[5] <= -0.55\\nentropy = 0.128\\nsamples = 29\\nvalue = [2, 27]'),\n",
       " Text(305.04, 108.72, 'X[6] <= 0.551\\nentropy = 0.5\\nsamples = 2\\nvalue = [1, 1]'),\n",
       " Text(300.78857142857146, 97.27578947368421, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(309.2914285714286, 97.27578947368421, 'entropy = 0.0\\nsamples = 1\\nvalue = [0, 1]'),\n",
       " Text(322.0457142857143, 108.72, 'X[7] <= 1.311\\nentropy = 0.071\\nsamples = 27\\nvalue = [1, 26]'),\n",
       " Text(317.79428571428576, 97.27578947368421, 'entropy = 0.0\\nsamples = 21\\nvalue = [0, 21]'),\n",
       " Text(326.2971428571429, 97.27578947368421, 'X[7] <= 1.335\\nentropy = 0.278\\nsamples = 6\\nvalue = [1, 5]'),\n",
       " Text(322.0457142857143, 85.83157894736843, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]'),\n",
       " Text(330.54857142857145, 85.83157894736843, 'entropy = 0.0\\nsamples = 5\\nvalue = [0, 5]'),\n",
       " Text(263.58857142857147, 165.94105263157894, 'entropy = 0.0\\nsamples = 1\\nvalue = [1, 0]')]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "tree.plot_tree(clf.fit(x_train,np.squeeze(y_train.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us limit the depth to 5 say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance are  [0.         0.02792399 0.         0.         0.04119422 0.14712109\n",
      " 0.05742869 0.04024308 0.06023461 0.60176289 0.02409144]\n",
      "train acc is  0.8613138686131386\n",
      "test acc is  0.7339901477832512\n"
     ]
    }
   ],
   "source": [
    "clf_dt = DecisionTreeClassifier(random_state=0,max_depth=5)\n",
    "# print(clf,end='\\n\\n')\n",
    "clf_dt=clf_dt.fit(x_train,np.squeeze(y_train.values))\n",
    "print('Feature Importance are ',clf_dt.feature_importances_)\n",
    "# we got some better features \n",
    "\n",
    "train_preds=clf_dt.predict(x_train)\n",
    "train_acc=accuracy_score(y_train, train_preds)\n",
    "print('train acc is ',train_acc)\n",
    "\n",
    "\n",
    "test_preds=clf_dt.predict(x_test)\n",
    "test_acc=accuracy_score(y_test,test_preds)\n",
    "print('test acc is ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see Decision tree has some potential of doing better, so that is ok\n",
    "\n",
    "Credit History is important feature\n",
    "\n",
    "##### Let us do Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 425 candidates, totalling 1275 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1213 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1275 out of 1275 | elapsed:   12.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                              criterion='gini', max_depth=5,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort=False, random_state=0,\n",
       "                                              splitter='best'),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': range(1, 18),\n",
       "                         'min_samples_split': range(10, 500, 20)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(accuracy_score), verbose=2)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "parameters = {'max_depth':range(1,18),'min_samples_split' : range(10,500,20)}\n",
    "clf = GridSearchCV(clf_dt, parameters, cv=3,verbose=2,n_jobs=-1,scoring=make_scorer(accuracy_score))\n",
    "clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01761643, 0.01483583, 0.01163848, 0.01471631, 0.01026511,\n",
       "        0.02626514, 0.00863775, 0.0121998 , 0.01414442, 0.01202313,\n",
       "        0.0105412 , 0.00995398, 0.01031009, 0.01031955, 0.00899752,\n",
       "        0.00972478, 0.01063848, 0.00966342, 0.01230653, 0.00931557,\n",
       "        0.00921059, 0.00945203, 0.01214369, 0.0123059 , 0.01271812,\n",
       "        0.00996319, 0.01027775, 0.00797892, 0.01033457, 0.00994007,\n",
       "        0.0085899 , 0.00916139, 0.01324415, 0.01060772, 0.01226862,\n",
       "        0.01031725, 0.01240555, 0.01274077, 0.01359399, 0.02608331,\n",
       "        0.01022951, 0.01174259, 0.01522374, 0.00926884, 0.01008018,\n",
       "        0.01399946, 0.01396457, 0.01060454, 0.01113343, 0.01263467,\n",
       "        0.01362888, 0.01282636, 0.01130978, 0.01027203, 0.01060351,\n",
       "        0.01050766, 0.01134491, 0.00991432, 0.01396894, 0.01397856,\n",
       "        0.01265176, 0.01201582, 0.0109609 , 0.01163554, 0.01063871,\n",
       "        0.01036302, 0.01382065, 0.01030755, 0.01063808, 0.00931025,\n",
       "        0.01116347, 0.00971921, 0.011964  , 0.01330185, 0.01108336,\n",
       "        0.01089462, 0.0117623 , 0.01195312, 0.01191489, 0.01542282,\n",
       "        0.01194406, 0.01271335, 0.01236224, 0.01102297, 0.01051267,\n",
       "        0.01100683, 0.01329954, 0.01054494, 0.01039847, 0.01094198,\n",
       "        0.01001986, 0.01206748, 0.00831079, 0.0116756 , 0.01080807,\n",
       "        0.01227037, 0.01163443, 0.0101916 , 0.00987609, 0.01372226,\n",
       "        0.01864409, 0.01170246, 0.01933376, 0.01017944, 0.01207534,\n",
       "        0.01399414, 0.01596022, 0.01130517, 0.01544627, 0.01223723,\n",
       "        0.01087809, 0.01319218, 0.01356959, 0.01131034, 0.01122324,\n",
       "        0.00961391, 0.01130549, 0.01053786, 0.01077549, 0.01208711,\n",
       "        0.01152651, 0.01162632, 0.00911077, 0.00977898, 0.01170174,\n",
       "        0.0129257 , 0.01201034, 0.01149122, 0.01268498, 0.0113163 ,\n",
       "        0.01229676, 0.00994452, 0.01234007, 0.01208933, 0.01196837,\n",
       "        0.01305858, 0.0110654 , 0.01030699, 0.00964228, 0.01075172,\n",
       "        0.01097616, 0.01103004, 0.01088953, 0.01196512, 0.01123635,\n",
       "        0.01040324, 0.0089759 , 0.01272257, 0.01219122, 0.01207908,\n",
       "        0.01059055, 0.0123059 , 0.01296973, 0.01325957, 0.01530067,\n",
       "        0.00964514, 0.01442417, 0.0147322 , 0.01194994, 0.01260781,\n",
       "        0.01263722, 0.01065389, 0.0113066 , 0.01182826, 0.00964173,\n",
       "        0.01162537, 0.01263507, 0.01396322, 0.0090936 , 0.01082365,\n",
       "        0.01276747, 0.01130478, 0.01102487, 0.01281873, 0.01112962,\n",
       "        0.01090654, 0.01263491, 0.01370088, 0.01322858, 0.01363508,\n",
       "        0.01226608, 0.01093507, 0.01394629, 0.01263277, 0.01118175,\n",
       "        0.0124619 , 0.01252588, 0.01213853, 0.01129913, 0.01320942,\n",
       "        0.01079281, 0.01125383, 0.01030644, 0.01041484, 0.0110081 ,\n",
       "        0.01035396, 0.01391037, 0.01067122, 0.01143106, 0.01014225,\n",
       "        0.01218653, 0.01413504, 0.01276088, 0.01128451, 0.0108331 ,\n",
       "        0.0145843 , 0.01095072, 0.010638  , 0.01582877, 0.01158341,\n",
       "        0.00949542, 0.01386388, 0.01093467, 0.01210809, 0.01133339,\n",
       "        0.01197076, 0.01303752, 0.00986687, 0.01529797, 0.01051323,\n",
       "        0.01232036, 0.01163705, 0.00969919, 0.01064046, 0.01149527,\n",
       "        0.01242836, 0.01123818, 0.01376136, 0.01041945, 0.01175412,\n",
       "        0.01496061, 0.01077978, 0.01095271, 0.01469318, 0.01077914,\n",
       "        0.01294708, 0.01171287, 0.01133259, 0.01023714, 0.01121696,\n",
       "        0.008871  , 0.01188723, 0.01096964, 0.01035126, 0.01288168,\n",
       "        0.01168609, 0.01133021, 0.00997496, 0.01247764, 0.0102702 ,\n",
       "        0.01339626, 0.01167226, 0.01296639, 0.01429947, 0.01163626,\n",
       "        0.01284091, 0.01193961, 0.01253915, 0.01121982, 0.01224271,\n",
       "        0.00979519, 0.01162124, 0.01197743, 0.01263634, 0.01093594,\n",
       "        0.01204435, 0.01352652, 0.0126346 , 0.01081157, 0.01436297,\n",
       "        0.01196774, 0.01162767, 0.00964785, 0.01374857, 0.01244942,\n",
       "        0.01263777, 0.01382931, 0.01300701, 0.01328651, 0.01152452,\n",
       "        0.01576161, 0.01124406, 0.01036533, 0.00918643, 0.01230097,\n",
       "        0.00975966, 0.01150576, 0.01035937, 0.01195645, 0.01417247,\n",
       "        0.00863512, 0.01294764, 0.01164357, 0.01396402, 0.0119915 ,\n",
       "        0.00977818, 0.01063776, 0.00932813, 0.01263316, 0.00828091,\n",
       "        0.01365153, 0.01196885, 0.01228833, 0.01228007, 0.0132978 ,\n",
       "        0.01264246, 0.01191401, 0.01322301, 0.01107915, 0.01137129,\n",
       "        0.00864363, 0.01061519, 0.01209831, 0.01196774, 0.01072709,\n",
       "        0.01214925, 0.012302  , 0.01046856, 0.01012317, 0.01114027,\n",
       "        0.01296743, 0.01044838, 0.00953722, 0.01246746, 0.01471361,\n",
       "        0.01392547, 0.01041357, 0.01278122, 0.01130128, 0.01196973,\n",
       "        0.01029269, 0.01311707, 0.01156386, 0.01429248, 0.01058594,\n",
       "        0.01284512, 0.00998513, 0.01113772, 0.01086028, 0.01160264,\n",
       "        0.01154653, 0.01163896, 0.0109152 , 0.01030556, 0.01050822,\n",
       "        0.01126401, 0.01066923, 0.01163069, 0.01503476, 0.01145959,\n",
       "        0.01329803, 0.01433078, 0.01160685, 0.01297228, 0.014889  ,\n",
       "        0.01196893, 0.0119795 , 0.01462396, 0.00931525, 0.01162179,\n",
       "        0.00997623, 0.01351555, 0.01163888, 0.01002383, 0.01083748,\n",
       "        0.01163363, 0.01028069, 0.00995692, 0.01361688, 0.00970006,\n",
       "        0.01127028, 0.01196933, 0.01130748, 0.01263269, 0.01195391,\n",
       "        0.01695546, 0.01396537, 0.01130271, 0.01396306, 0.01359622,\n",
       "        0.01099284, 0.01113089, 0.01198284, 0.01163125, 0.01262259,\n",
       "        0.01495075, 0.01502538, 0.01793083, 0.01132305, 0.01189677,\n",
       "        0.01229795, 0.01105944, 0.01012786, 0.01275969, 0.00998672,\n",
       "        0.00929904, 0.01026026, 0.01031351, 0.01290766, 0.00924699,\n",
       "        0.01448115, 0.01198045, 0.01000174, 0.0116686 , 0.01232481,\n",
       "        0.01140682, 0.01466425, 0.01071533, 0.01239189, 0.01496673,\n",
       "        0.01132417, 0.01329724, 0.01867326, 0.01242137, 0.01079631,\n",
       "        0.011554  , 0.01248686, 0.01227411, 0.009775  , 0.01196583,\n",
       "        0.01026154, 0.01362832, 0.00859968, 0.01084487, 0.01028768]),\n",
       " 'std_fit_time': array([7.28566229e-03, 3.62240931e-03, 1.29247644e-03, 4.04838137e-03,\n",
       "        9.09521746e-04, 2.02425798e-02, 1.53102591e-03, 2.37400477e-03,\n",
       "        5.34412150e-03, 8.09725103e-04, 4.09805267e-04, 1.12362082e-03,\n",
       "        1.24116273e-03, 1.23772133e-03, 8.35028511e-04, 1.58848713e-03,\n",
       "        1.69543531e-03, 1.67707493e-03, 3.67885887e-03, 4.76034615e-04,\n",
       "        5.51813745e-04, 1.44172955e-03, 1.89356453e-03, 6.91533688e-03,\n",
       "        5.32199428e-03, 1.64202420e-03, 1.01263409e-03, 8.14004070e-04,\n",
       "        9.60730562e-04, 1.04833121e-03, 1.56169273e-03, 1.66635747e-03,\n",
       "        3.32204739e-03, 9.62896126e-04, 4.25627019e-04, 1.26106908e-03,\n",
       "        1.03101167e-03, 1.09153246e-03, 1.22254629e-03, 2.24818249e-02,\n",
       "        1.32814837e-03, 4.96362212e-04, 4.97285906e-03, 6.65270161e-04,\n",
       "        1.35692628e-03, 5.77737295e-03, 5.64284508e-03, 4.47588168e-04,\n",
       "        6.26062765e-04, 4.71314972e-04, 1.69476857e-03, 1.57608846e-03,\n",
       "        4.65591109e-04, 1.66700976e-03, 1.23458290e-03, 1.78369929e-03,\n",
       "        2.53367830e-03, 1.01844537e-03, 2.15370007e-03, 7.68323377e-04,\n",
       "        1.20759311e-03, 1.68833003e-03, 1.41806754e-03, 1.24464377e-03,\n",
       "        1.88042429e-03, 9.92391197e-04, 2.08841998e-03, 1.88177299e-03,\n",
       "        1.69545090e-03, 1.24426179e-03, 2.30822128e-03, 1.50806874e-03,\n",
       "        1.40776097e-03, 3.29487203e-03, 6.81172298e-04, 1.08458065e-04,\n",
       "        1.57745607e-03, 2.46072038e-03, 1.45093027e-03, 1.74905231e-03,\n",
       "        1.62908525e-03, 1.61496687e-03, 1.01526539e-03, 1.58853146e-03,\n",
       "        1.04925443e-03, 1.61769370e-03, 1.87901981e-03, 1.14459398e-03,\n",
       "        6.80336243e-04, 8.23280884e-04, 8.53801251e-04, 7.04329599e-04,\n",
       "        9.42917740e-04, 5.02272560e-04, 1.28133455e-03, 4.88833690e-04,\n",
       "        4.68404365e-04, 1.72811020e-03, 1.04759480e-03, 3.04867536e-03,\n",
       "        7.42925037e-03, 2.43115653e-03, 9.60631667e-03, 2.49277263e-04,\n",
       "        1.34107575e-03, 3.25850768e-03, 5.08945649e-03, 9.37415250e-04,\n",
       "        2.68122767e-03, 1.93734008e-03, 1.49445685e-03, 2.71818305e-03,\n",
       "        3.30639677e-03, 4.77776714e-04, 1.19426412e-03, 1.92133469e-03,\n",
       "        2.48455128e-03, 1.20789212e-03, 2.20098109e-03, 1.48760859e-03,\n",
       "        2.33871942e-03, 9.46629358e-04, 9.76291997e-04, 6.95373962e-04,\n",
       "        2.94412047e-03, 2.84884433e-03, 2.21396048e-03, 1.89148910e-03,\n",
       "        1.26040822e-03, 1.26205295e-03, 2.05429713e-03, 5.54733085e-05,\n",
       "        1.32939584e-03, 1.73182213e-03, 2.15350516e-03, 8.90809586e-04,\n",
       "        2.56328675e-03, 1.69484304e-03, 4.70471583e-04, 5.67545333e-04,\n",
       "        8.04861958e-04, 2.08644320e-03, 8.19962107e-04, 1.62460415e-03,\n",
       "        2.12694776e-03, 4.18212300e-04, 8.16437433e-04, 2.05979358e-03,\n",
       "        1.59216599e-03, 7.89519308e-04, 9.84859583e-04, 2.04639419e-03,\n",
       "        4.55290018e-06, 1.35693144e-03, 2.04784314e-03, 4.73382128e-04,\n",
       "        1.90666938e-03, 4.00989013e-03, 1.42175679e-03, 1.72164710e-03,\n",
       "        1.24664280e-03, 2.00777649e-03, 1.24290201e-03, 8.22027254e-04,\n",
       "        1.24460184e-03, 2.47677694e-03, 2.05227164e-03, 4.23036349e-03,\n",
       "        1.33615917e-03, 2.07812152e-04, 3.01144283e-03, 4.70921149e-04,\n",
       "        1.33684482e-03, 1.43095325e-03, 1.80166777e-03, 1.54355227e-03,\n",
       "        4.71827791e-04, 1.32581792e-03, 8.94671901e-04, 1.24437849e-03,\n",
       "        1.24412060e-03, 8.16317083e-04, 2.13960574e-03, 1.24525975e-03,\n",
       "        8.73184979e-04, 7.50162404e-04, 2.31448698e-03, 1.33503632e-03,\n",
       "        1.11707900e-03, 1.66448184e-03, 1.39237845e-03, 4.16524604e-04,\n",
       "        9.39762577e-04, 4.16159522e-04, 8.09414662e-04, 1.90443484e-03,\n",
       "        1.47873993e-03, 4.23042147e-04, 7.49258632e-04, 4.84930509e-04,\n",
       "        4.31431754e-04, 1.42207647e-03, 1.60832524e-03, 9.28722321e-04,\n",
       "        9.88530052e-04, 1.16489616e-03, 2.00558414e-03, 1.24330994e-03,\n",
       "        5.05052274e-03, 7.88437236e-04, 6.00460567e-04, 6.06387526e-03,\n",
       "        1.38377426e-03, 6.51213134e-04, 2.60387723e-03, 8.10892327e-04,\n",
       "        4.18669640e-03, 8.29221174e-04, 3.66759187e-03, 6.47487985e-04,\n",
       "        1.22288609e-03, 2.86009254e-03, 1.77082720e-03, 4.68729161e-04,\n",
       "        1.22178386e-03, 1.56857208e-03, 8.98070968e-04, 2.27139735e-03,\n",
       "        1.12636619e-03, 2.20332868e-03, 4.53359877e-03, 1.99277274e-03,\n",
       "        1.63011603e-03, 3.10612046e-03, 1.05896832e-03, 1.43957276e-03,\n",
       "        1.34632783e-03, 3.38429652e-03, 1.84616739e-03, 3.50605705e-04,\n",
       "        1.31146812e-03, 8.50964736e-04, 2.93442049e-03, 4.41912822e-04,\n",
       "        1.17875201e-03, 9.06826332e-04, 1.28012157e-03, 8.16437680e-04,\n",
       "        3.93728102e-04, 8.62808527e-04, 2.58820097e-03, 6.16330058e-04,\n",
       "        1.40955923e-03, 3.22999439e-03, 2.48782180e-03, 2.77109423e-03,\n",
       "        2.14033306e-03, 5.75944513e-04, 2.86209585e-04, 1.73087788e-03,\n",
       "        1.30237420e-03, 1.66390750e-03, 8.04283535e-04, 2.05244059e-03,\n",
       "        7.37236750e-04, 2.35147227e-03, 6.26555085e-04, 1.24504817e-03,\n",
       "        1.01885981e-03, 1.23584954e-03, 3.54983618e-03, 4.65518244e-04,\n",
       "        1.70530238e-03, 4.92252195e-03, 1.11891781e-03, 1.24717580e-03,\n",
       "        3.34997103e-03, 2.24983792e-03, 3.27318054e-03, 1.03098217e-03,\n",
       "        2.94557737e-03, 1.97262954e-03, 4.34898595e-04, 5.78154201e-04,\n",
       "        4.01614307e-03, 1.51385030e-03, 2.03344069e-03, 1.95151480e-03,\n",
       "        8.05730664e-04, 3.45403272e-03, 4.64629033e-04, 7.91442084e-04,\n",
       "        4.76565217e-04, 4.95315355e-03, 1.65998323e-03, 1.13352250e-03,\n",
       "        9.36671561e-04, 1.21455396e-03, 1.69526387e-03, 1.20041050e-03,\n",
       "        3.31918169e-03, 2.44327752e-03, 2.87465051e-03, 1.26547770e-03,\n",
       "        4.48414506e-03, 1.24477918e-03, 1.37199058e-03, 8.87960102e-04,\n",
       "        1.55612094e-03, 1.75360641e-03, 4.71035753e-04, 5.15486674e-04,\n",
       "        7.92592675e-05, 2.93639182e-03, 1.33122147e-03, 6.43708303e-04,\n",
       "        2.61359919e-03, 1.80580352e-03, 2.11956044e-03, 1.54249189e-03,\n",
       "        2.81990611e-03, 6.71541169e-04, 7.92192171e-04, 1.07493199e-03,\n",
       "        3.96573586e-03, 2.10088616e-03, 1.22852652e-03, 9.92556338e-04,\n",
       "        1.23954607e-03, 8.15366777e-04, 4.89385533e-04, 2.35795674e-03,\n",
       "        4.28905121e-04, 4.67998768e-04, 1.74741786e-03, 8.23368413e-04,\n",
       "        6.01553379e-05, 1.59826270e-03, 1.81198506e-03, 1.63123045e-03,\n",
       "        1.30372722e-03, 4.48551198e-03, 2.12831494e-03, 1.24268957e-03,\n",
       "        1.41795698e-03, 1.28149721e-03, 1.30181576e-03, 9.48561640e-04,\n",
       "        2.75844833e-03, 1.82074985e-03, 9.40212023e-04, 1.20577520e-03,\n",
       "        1.22672512e-03, 1.41508945e-03, 1.46094549e-03, 2.15527101e-03,\n",
       "        8.21593729e-04, 2.49266965e-03, 1.87571305e-03, 7.80254426e-04,\n",
       "        8.10501901e-04, 2.33525986e-03, 4.67157950e-04, 8.17250550e-04,\n",
       "        6.59715204e-04, 9.35997718e-04, 9.78202818e-04, 8.14273887e-04,\n",
       "        3.28196991e-03, 5.94434561e-04, 9.22438309e-04, 2.15442504e-03,\n",
       "        4.71715402e-04, 2.85917798e-03, 8.14037620e-04, 7.09717136e-03,\n",
       "        2.93766005e-03, 2.48877754e-03, 4.95233752e-03, 2.69462998e-03,\n",
       "        1.60266405e-03, 1.31210398e-03, 2.17141237e-03, 1.25006444e-03,\n",
       "        1.24442938e-03, 6.36717683e-03, 1.51808941e-03, 8.93260872e-03,\n",
       "        4.96265331e-04, 7.06114216e-04, 2.62039449e-03, 1.41043124e-04,\n",
       "        1.02329284e-03, 1.08853312e-03, 1.62927167e-03, 4.78063001e-04,\n",
       "        2.10184953e-03, 1.23510242e-03, 1.63199820e-03, 1.04182867e-03,\n",
       "        2.14033000e-03, 8.28947384e-04, 6.60083221e-05, 9.15755020e-04,\n",
       "        2.45401827e-03, 4.31991848e-04, 1.74225241e-03, 5.89584170e-04,\n",
       "        1.81833848e-03, 3.70044582e-03, 4.60201838e-04, 1.24466489e-03,\n",
       "        1.03682716e-02, 1.91590157e-03, 1.30507608e-03, 1.34040007e-03,\n",
       "        3.65630878e-03, 2.64312771e-03, 5.77777649e-04, 2.68676593e-03,\n",
       "        5.41336966e-04, 4.67728656e-04, 2.23426749e-03, 2.75437775e-04,\n",
       "        9.77263560e-04]),\n",
       " 'mean_score_time': array([0.00523853, 0.00635839, 0.00428049, 0.00636609, 0.00496078,\n",
       "        0.01373776, 0.00308839, 0.00399319, 0.00565211, 0.00331759,\n",
       "        0.00299605, 0.00294391, 0.00298834, 0.00264645, 0.00312201,\n",
       "        0.00329351, 0.0033257 , 0.00332642, 0.00564718, 0.00371861,\n",
       "        0.00664719, 0.00317844, 0.00308808, 0.00337323, 0.00352923,\n",
       "        0.00313656, 0.00344245, 0.00302617, 0.00396609, 0.00269214,\n",
       "        0.00393295, 0.00362833, 0.00373324, 0.00542116, 0.00432388,\n",
       "        0.00331346, 0.00384633, 0.0038739 , 0.00664409, 0.00432348,\n",
       "        0.00376169, 0.00321968, 0.00368444, 0.00298675, 0.00332872,\n",
       "        0.00470511, 0.00398771, 0.00406384, 0.00266083, 0.00425196,\n",
       "        0.00440057, 0.00346343, 0.00398882, 0.00334136, 0.0050168 ,\n",
       "        0.0034465 , 0.00348147, 0.00328732, 0.0033501 , 0.002285  ,\n",
       "        0.00429646, 0.00432261, 0.00289623, 0.00286142, 0.00299096,\n",
       "        0.00331887, 0.00312312, 0.00366735, 0.00399089, 0.00400543,\n",
       "        0.00349879, 0.00317399, 0.00265996, 0.00367832, 0.00332332,\n",
       "        0.0029796 , 0.00371242, 0.0033253 , 0.00337481, 0.00366187,\n",
       "        0.00365686, 0.00400019, 0.00365249, 0.00392715, 0.00397031,\n",
       "        0.00362595, 0.0041697 , 0.0043238 , 0.00439024, 0.00365806,\n",
       "        0.00358613, 0.00324543, 0.00343911, 0.00400798, 0.00375954,\n",
       "        0.00731611, 0.00299382, 0.00344157, 0.00329908, 0.00420014,\n",
       "        0.00529257, 0.00412456, 0.00465552, 0.00364415, 0.00329733,\n",
       "        0.00362166, 0.00363922, 0.00406313, 0.00479595, 0.00353956,\n",
       "        0.00370749, 0.00365249, 0.00384784, 0.00298556, 0.00519093,\n",
       "        0.0032963 , 0.00332308, 0.00280062, 0.00332602, 0.00333079,\n",
       "        0.00465202, 0.00399176, 0.00401394, 0.0033083 , 0.00365742,\n",
       "        0.00432165, 0.00415373, 0.00465488, 0.00360672, 0.00297952,\n",
       "        0.00432046, 0.00332991, 0.00368849, 0.00345286, 0.00500798,\n",
       "        0.00285753, 0.0028766 , 0.00370518, 0.00353456, 0.00428494,\n",
       "        0.00307576, 0.00322739, 0.00305438, 0.00338562, 0.00332085,\n",
       "        0.00522987, 0.00372593, 0.00441543, 0.00276875, 0.0039227 ,\n",
       "        0.00371591, 0.00331616, 0.00431832, 0.00491778, 0.00344857,\n",
       "        0.00432046, 0.00352891, 0.00354218, 0.00293771, 0.00332483,\n",
       "        0.00565052, 0.0045855 , 0.00299915, 0.00432221, 0.0059847 ,\n",
       "        0.00332427, 0.00465242, 0.00498637, 0.00434526, 0.00424957,\n",
       "        0.00341622, 0.00366759, 0.00363088, 0.00370773, 0.00381907,\n",
       "        0.00331378, 0.00440645, 0.00391984, 0.00332514, 0.00398572,\n",
       "        0.00373316, 0.00366259, 0.00366608, 0.00399152, 0.00332109,\n",
       "        0.00413871, 0.00296307, 0.00320323, 0.00373054, 0.00331314,\n",
       "        0.00376089, 0.00365718, 0.00440868, 0.00468453, 0.00431871,\n",
       "        0.00393271, 0.00332236, 0.00362476, 0.00319147, 0.00373292,\n",
       "        0.00265193, 0.00265948, 0.00383202, 0.00468286, 0.00396578,\n",
       "        0.00359948, 0.00395091, 0.00332626, 0.00299191, 0.00431943,\n",
       "        0.00425291, 0.00371774, 0.00403261, 0.00386858, 0.00286174,\n",
       "        0.00764354, 0.0043815 , 0.00354163, 0.00431736, 0.00495815,\n",
       "        0.00407283, 0.00299438, 0.00359678, 0.00265916, 0.00342115,\n",
       "        0.00332085, 0.00581002, 0.004855  , 0.00425673, 0.00332355,\n",
       "        0.00698145, 0.00368071, 0.00432102, 0.01884071, 0.0031848 ,\n",
       "        0.00398501, 0.00351675, 0.00537467, 0.00262904, 0.00266178,\n",
       "        0.00393581, 0.00390967, 0.00365718, 0.00591493, 0.00471918,\n",
       "        0.00327444, 0.0035224 , 0.00374111, 0.00396617, 0.005066  ,\n",
       "        0.00425887, 0.00309253, 0.00432094, 0.00428605, 0.00432142,\n",
       "        0.00306869, 0.00531729, 0.00332737, 0.00360878, 0.00371091,\n",
       "        0.00314625, 0.00306066, 0.00331839, 0.00306575, 0.00397722,\n",
       "        0.00291816, 0.00398612, 0.00465608, 0.00371075, 0.00425688,\n",
       "        0.0066541 , 0.00399677, 0.00349156, 0.00441511, 0.00432722,\n",
       "        0.00298762, 0.00366767, 0.00355315, 0.00500679, 0.00366974,\n",
       "        0.00399025, 0.00333158, 0.00359996, 0.00374126, 0.00415667,\n",
       "        0.00344165, 0.00465353, 0.00393828, 0.00299335, 0.00317995,\n",
       "        0.00370121, 0.00569248, 0.00331664, 0.00365631, 0.00330257,\n",
       "        0.00359654, 0.00332332, 0.00296021, 0.00301282, 0.00299072,\n",
       "        0.00363175, 0.00299637, 0.00333087, 0.0036664 , 0.00332697,\n",
       "        0.00366783, 0.00465806, 0.00398755, 0.00310596, 0.00480541,\n",
       "        0.00508722, 0.00332046, 0.00310922, 0.00302609, 0.00290386,\n",
       "        0.00397221, 0.00301941, 0.00386453, 0.00434955, 0.00366163,\n",
       "        0.00498565, 0.00394599, 0.00442831, 0.00432086, 0.00322707,\n",
       "        0.00465798, 0.00465337, 0.00448362, 0.00303117, 0.00365472,\n",
       "        0.0046196 , 0.00269628, 0.00399335, 0.0046734 , 0.00365257,\n",
       "        0.00346843, 0.00305502, 0.00432007, 0.00367983, 0.00348973,\n",
       "        0.00398914, 0.00265741, 0.00365305, 0.00304826, 0.00299001,\n",
       "        0.0026602 , 0.00348473, 0.00498748, 0.00464122, 0.00260941,\n",
       "        0.00332626, 0.00344364, 0.00364884, 0.00534829, 0.00400607,\n",
       "        0.00399105, 0.00339556, 0.00681702, 0.00464829, 0.00386834,\n",
       "        0.00569638, 0.00299191, 0.00412814, 0.00493685, 0.00332475,\n",
       "        0.00375493, 0.00496316, 0.00365925, 0.00392858, 0.00688982,\n",
       "        0.00382694, 0.00465473, 0.00404962, 0.0070099 , 0.00299191,\n",
       "        0.0039893 , 0.00371257, 0.00313727, 0.0046548 , 0.00497572,\n",
       "        0.0032564 , 0.00383862, 0.00339937, 0.00272099, 0.00269675,\n",
       "        0.00299239, 0.00456103, 0.00433222, 0.00400297, 0.002695  ,\n",
       "        0.00531713, 0.00292102, 0.00349522, 0.00456023, 0.00297983,\n",
       "        0.00434049, 0.0033563 , 0.00321492, 0.00271161, 0.00359718,\n",
       "        0.0043207 , 0.00331171, 0.00381104, 0.00344547, 0.00460664,\n",
       "        0.00363668, 0.00395044, 0.00364463, 0.00224686, 0.00491818,\n",
       "        0.00397062, 0.00503039, 0.00489632, 0.00340056, 0.00320848,\n",
       "        0.00372458, 0.00595252, 0.00365655, 0.00557343, 0.00350412,\n",
       "        0.00298119, 0.00266107, 0.00364296, 0.00433397, 0.00300614]),\n",
       " 'std_score_time': array([5.32093331e-04, 2.67287444e-03, 9.12957308e-04, 2.67546255e-03,\n",
       "        2.16524826e-03, 1.42679213e-02, 1.13792524e-03, 8.16340627e-04,\n",
       "        3.75550905e-03, 4.70256336e-04, 8.19164981e-04, 6.85588736e-05,\n",
       "        2.54561383e-06, 4.62152671e-04, 1.29783232e-03, 4.92322878e-04,\n",
       "        4.69685125e-04, 1.21214896e-03, 2.48942394e-03, 9.00021536e-04,\n",
       "        5.17338522e-03, 8.56344362e-04, 6.75518569e-04, 9.78355403e-04,\n",
       "        1.41868466e-03, 9.98024792e-04, 6.35632348e-04, 8.16152227e-04,\n",
       "        7.83914487e-04, 9.01538264e-04, 8.18955106e-04, 4.51405493e-04,\n",
       "        1.06777239e-03, 1.81565127e-03, 1.24517470e-03, 4.79477342e-04,\n",
       "        8.35353862e-04, 1.46612931e-04, 4.09104427e-03, 4.68841634e-04,\n",
       "        1.08626479e-03, 3.80836216e-04, 4.42028060e-04, 8.22768350e-04,\n",
       "        4.49683031e-04, 1.51269069e-03, 8.16541467e-04, 9.66723885e-05,\n",
       "        4.71428248e-04, 7.79675634e-04, 5.79720559e-04, 5.87749248e-04,\n",
       "        8.12742822e-04, 8.57153584e-04, 1.63069421e-03, 1.02615789e-03,\n",
       "        1.48141578e-03, 5.08740221e-04, 4.51705432e-04, 5.01323505e-04,\n",
       "        9.75730209e-04, 3.71278403e-04, 8.09428708e-04, 6.58968547e-04,\n",
       "        8.15269673e-04, 1.39249726e-03, 6.58295222e-04, 9.32566645e-04,\n",
       "        1.62800824e-03, 8.35113977e-04, 2.93574766e-04, 6.06679185e-04,\n",
       "        9.39425281e-04, 3.09118185e-04, 4.71370354e-04, 8.31970308e-04,\n",
       "        9.02796771e-04, 4.69965550e-04, 4.35693281e-04, 4.71753961e-04,\n",
       "        4.71033340e-04, 7.97057974e-04, 4.28474973e-04, 8.33384912e-04,\n",
       "        7.91132492e-04, 4.67321553e-04, 2.40555051e-03, 1.24150370e-03,\n",
       "        5.16313622e-04, 4.71375218e-04, 4.28478334e-04, 1.11657115e-03,\n",
       "        4.97011510e-04, 8.14578405e-04, 1.01324033e-03, 5.41919507e-03,\n",
       "        1.73024663e-06, 4.12962590e-04, 1.29721667e-04, 1.37659808e-03,\n",
       "        4.91215581e-04, 2.93871400e-04, 4.72665753e-04, 1.22639578e-03,\n",
       "        2.24393642e-04, 4.49980118e-04, 9.58222590e-04, 9.17734216e-04,\n",
       "        1.05644104e-03, 1.11250622e-03, 3.56787985e-04, 1.23901909e-03,\n",
       "        2.22766451e-03, 5.96946461e-06, 1.12369133e-03, 4.29954150e-04,\n",
       "        1.24381860e-03, 2.90367098e-04, 4.70808758e-04, 4.78395631e-04,\n",
       "        1.51097956e-03, 1.41203198e-03, 1.06194401e-03, 1.24627708e-03,\n",
       "        4.71426801e-04, 4.72157135e-04, 9.37520728e-04, 9.39202193e-04,\n",
       "        4.39241930e-04, 8.30908576e-04, 4.65077891e-04, 1.21967981e-03,\n",
       "        1.45314618e-03, 4.10536271e-04, 1.39612600e-03, 8.36544768e-04,\n",
       "        1.64600975e-04, 5.07927504e-04, 4.13525508e-04, 3.39195956e-04,\n",
       "        9.26354443e-04, 5.52934522e-04, 8.36773359e-05, 4.33238960e-04,\n",
       "        4.71265843e-04, 5.71669962e-04, 1.03720585e-03, 8.07311674e-04,\n",
       "        8.72347832e-04, 6.56710289e-04, 5.15929921e-04, 4.60414488e-04,\n",
       "        1.24851464e-03, 9.00586250e-04, 4.10906626e-04, 1.25373865e-03,\n",
       "        4.10418238e-04, 4.12283198e-04, 7.69921810e-05, 9.42970635e-04,\n",
       "        2.34521958e-03, 8.41251659e-04, 1.40718587e-05, 1.24313859e-03,\n",
       "        1.40882932e-03, 4.72551678e-04, 9.38245044e-04, 8.14199420e-04,\n",
       "        9.56366243e-04, 5.29261000e-04, 6.09850502e-04, 9.31570698e-04,\n",
       "        4.63498268e-04, 9.06666293e-04, 6.38124238e-04, 4.68417700e-04,\n",
       "        4.24425966e-04, 9.09706326e-04, 4.69403511e-04, 8.20334487e-04,\n",
       "        5.50742411e-04, 4.70542729e-04, 2.57105473e-04, 1.40978439e-03,\n",
       "        4.59123013e-04, 9.99826611e-04, 1.28037252e-03, 1.08432484e-03,\n",
       "        1.34596497e-03, 4.53950032e-04, 2.62649157e-04, 1.24268991e-03,\n",
       "        1.78140487e-03, 5.78917765e-04, 8.79677908e-04, 1.36893574e-03,\n",
       "        4.76324640e-04, 9.64945023e-04, 5.87532881e-04, 9.36265622e-04,\n",
       "        4.64301600e-04, 4.68616770e-04, 7.51137102e-04, 4.92366571e-04,\n",
       "        7.85938118e-04, 4.34400672e-04, 2.77252094e-05, 1.24360291e-03,\n",
       "        8.48537942e-07, 1.30615026e-03, 1.78264312e-03, 1.30797039e-03,\n",
       "        8.68177437e-04, 1.71650228e-04, 6.60383858e-04, 5.89136529e-03,\n",
       "        4.31517050e-04, 8.99873468e-04, 9.36235676e-04, 2.12863769e-03,\n",
       "        9.66517874e-04, 2.51063808e-06, 9.83784358e-04, 4.69235923e-04,\n",
       "        1.37291656e-03, 4.65357658e-04, 1.30519468e-03, 2.98007937e-03,\n",
       "        5.23315367e-04, 5.76041885e-04, 5.64070959e-03, 1.72649946e-03,\n",
       "        9.38807183e-04, 1.75150095e-02, 5.94295824e-04, 8.16340766e-04,\n",
       "        5.06222641e-04, 3.30356702e-03, 4.49088086e-04, 4.71093005e-04,\n",
       "        6.91876120e-04, 9.31950389e-04, 1.69446914e-03, 1.53963737e-03,\n",
       "        1.16208888e-03, 3.96349931e-04, 4.03874599e-04, 1.26558927e-03,\n",
       "        2.04913645e-05, 1.43138303e-03, 5.20340160e-04, 1.38916558e-04,\n",
       "        4.69796912e-04, 4.91185788e-04, 4.69628294e-04, 7.02777982e-04,\n",
       "        1.24169178e-03, 4.78116386e-04, 5.38018887e-04, 9.13152959e-04,\n",
       "        2.25361630e-04, 8.23998187e-04, 4.77317978e-04, 8.28211658e-04,\n",
       "        8.17142185e-04, 1.03962409e-04, 8.08363762e-04, 4.68672996e-04,\n",
       "        1.67435551e-03, 5.25315915e-04, 4.49228033e-03, 8.15793461e-04,\n",
       "        4.08629197e-04, 1.14768761e-03, 9.36446960e-04, 5.78461977e-06,\n",
       "        5.88095671e-04, 1.34851185e-03, 2.96377809e-03, 1.71489117e-03,\n",
       "        1.07214749e-06, 4.79014406e-04, 5.55216157e-04, 3.52011340e-04,\n",
       "        2.37319346e-04, 1.12438172e-03, 2.35072644e-03, 1.68724947e-03,\n",
       "        8.14490792e-04, 8.57834274e-04, 6.23792221e-04, 1.00225263e-03,\n",
       "        9.34661183e-04, 9.40270108e-04, 9.24307618e-04, 1.27269602e-03,\n",
       "        4.70696165e-04, 7.75255029e-04, 8.38562179e-04, 7.01885292e-07,\n",
       "        5.00314608e-04, 8.18388905e-04, 4.69266570e-04, 9.52013417e-04,\n",
       "        4.73002926e-04, 9.33223739e-04, 4.72615682e-04, 8.16729560e-04,\n",
       "        4.01709451e-04, 1.39212639e-03, 2.11305344e-03, 4.77190046e-04,\n",
       "        4.24706055e-04, 4.69796871e-05, 1.19477976e-04, 7.95484922e-04,\n",
       "        4.02404294e-05, 1.24670488e-03, 1.21478544e-03, 4.73130282e-04,\n",
       "        1.41354996e-03, 6.24973073e-05, 6.16249147e-04, 1.24292323e-03,\n",
       "        5.51009574e-04, 1.70329147e-03, 1.69387671e-03, 1.08197059e-03,\n",
       "        5.40147743e-05, 4.71033823e-04, 1.71317554e-03, 5.03120976e-04,\n",
       "        8.14107469e-04, 2.37612726e-03, 8.02134947e-04, 4.53553121e-04,\n",
       "        8.20280506e-04, 4.59125613e-04, 4.85995648e-04, 4.60056723e-04,\n",
       "        1.43491984e-06, 4.70194685e-04, 4.72217715e-04, 7.48046827e-04,\n",
       "        3.46049326e-06, 4.71314248e-04, 7.10769678e-04, 1.40484024e-03,\n",
       "        2.32858724e-03, 5.36388972e-04, 4.71482786e-04, 4.11725363e-04,\n",
       "        4.65097324e-04, 1.65136313e-03, 7.93855736e-04, 8.15271377e-04,\n",
       "        1.08956174e-03, 2.68620060e-03, 4.78510794e-04, 6.63734572e-04,\n",
       "        3.73000100e-03, 8.15756081e-04, 2.09696746e-03, 6.90762260e-05,\n",
       "        4.69684641e-04, 5.50345772e-04, 7.85719439e-04, 4.68335821e-04,\n",
       "        1.36886647e-03, 2.24165221e-03, 6.81021080e-04, 9.39482642e-04,\n",
       "        1.47633390e-03, 4.29606530e-03, 8.13906998e-04, 8.15366730e-04,\n",
       "        9.01769739e-04, 2.04217015e-04, 1.24373089e-03, 2.82816161e-03,\n",
       "        8.94909238e-04, 1.02386188e-03, 6.21881063e-04, 5.17622140e-04,\n",
       "        4.93194212e-04, 8.14198738e-04, 4.18652947e-04, 1.32750999e-03,\n",
       "        2.21754000e-03, 9.16698164e-04, 1.88081773e-03, 8.70025022e-04,\n",
       "        4.07490390e-04, 4.38902249e-04, 1.72523846e-05, 4.57590882e-04,\n",
       "        5.15149538e-04, 2.95836394e-04, 5.10620292e-04, 4.44980301e-04,\n",
       "        1.24398758e-03, 4.79153741e-04, 6.44878386e-04, 3.63793176e-04,\n",
       "        4.34731783e-04, 4.55877327e-04, 4.74548144e-05, 4.88341640e-04,\n",
       "        5.37314271e-04, 1.93700908e-04, 8.30368637e-04, 2.21390458e-03,\n",
       "        1.74160956e-03, 5.78086248e-04, 3.04413400e-04, 5.24344884e-04,\n",
       "        3.57655820e-03, 9.40102654e-04, 3.46326620e-04, 6.83857894e-04,\n",
       "        2.67928459e-05, 4.70246599e-04, 4.61724580e-04, 1.32358087e-03,\n",
       "        8.29729822e-04]),\n",
       " 'param_max_depth': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "                    8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "                    11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "                    11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13,\n",
       "                    13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "                    13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14,\n",
       "                    14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "                    14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "                    15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "                    15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                    16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "                    17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "                    17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230,\n",
       "                    250, 270, 290, 310, 330, 350, 370, 390, 410, 430, 450,\n",
       "                    470, 490, 10, 30, 50, 70, 90, 110, 130, 150, 170, 190,\n",
       "                    210, 230, 250, 270, 290, 310, 330, 350, 370, 390, 410,\n",
       "                    430, 450, 470, 490, 10, 30, 50, 70, 90, 110, 130, 150,\n",
       "                    170, 190, 210, 230, 250, 270, 290, 310, 330, 350, 370,\n",
       "                    390, 410, 430, 450, 470, 490, 10, 30, 50, 70, 90, 110,\n",
       "                    130, 150, 170, 190, 210, 230, 250, 270, 290, 310, 330,\n",
       "                    350, 370, 390, 410, 430, 450, 470, 490, 10, 30, 50, 70,\n",
       "                    90, 110, 130, 150, 170, 190, 210, 230, 250, 270, 290,\n",
       "                    310, 330, 350, 370, 390, 410, 430, 450, 470, 490, 10,\n",
       "                    30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250,\n",
       "                    270, 290, 310, 330, 350, 370, 390, 410, 430, 450, 470,\n",
       "                    490, 10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210,\n",
       "                    230, 250, 270, 290, 310, 330, 350, 370, 390, 410, 430,\n",
       "                    450, 470, 490, 10, 30, 50, 70, 90, 110, 130, 150, 170,\n",
       "                    190, 210, 230, 250, 270, 290, 310, 330, 350, 370, 390,\n",
       "                    410, 430, 450, 470, 490, 10, 30, 50, 70, 90, 110, 130,\n",
       "                    150, 170, 190, 210, 230, 250, 270, 290, 310, 330, 350,\n",
       "                    370, 390, 410, 430, 450, 470, 490, 10, 30, 50, 70, 90,\n",
       "                    110, 130, 150, 170, 190, 210, 230, 250, 270, 290, 310,\n",
       "                    330, 350, 370, 390, 410, 430, 450, 470, 490, 10, 30,\n",
       "                    50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250,\n",
       "                    270, 290, 310, 330, 350, 370, 390, 410, 430, 450, 470,\n",
       "                    490, 10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210,\n",
       "                    230, 250, 270, 290, 310, 330, 350, 370, 390, 410, 430,\n",
       "                    450, 470, 490, 10, 30, 50, 70, 90, 110, 130, 150, 170,\n",
       "                    190, 210, 230, 250, 270, 290, 310, 330, 350, 370, 390,\n",
       "                    410, 430, 450, 470, 490, 10, 30, 50, 70, 90, 110, 130,\n",
       "                    150, 170, 190, 210, 230, 250, 270, 290, 310, 330, 350,\n",
       "                    370, 390, 410, 430, 450, 470, 490, 10, 30, 50, 70, 90,\n",
       "                    110, 130, 150, 170, 190, 210, 230, 250, 270, 290, 310,\n",
       "                    330, 350, 370, 390, 410, 430, 450, 470, 490, 10, 30,\n",
       "                    50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250,\n",
       "                    270, 290, 310, 330, 350, 370, 390, 410, 430, 450, 470,\n",
       "                    490, 10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210,\n",
       "                    230, 250, 270, 290, 310, 330, 350, 370, 390, 410, 430,\n",
       "                    450, 470, 490],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 1, 'min_samples_split': 10},\n",
       "  {'max_depth': 1, 'min_samples_split': 30},\n",
       "  {'max_depth': 1, 'min_samples_split': 50},\n",
       "  {'max_depth': 1, 'min_samples_split': 70},\n",
       "  {'max_depth': 1, 'min_samples_split': 90},\n",
       "  {'max_depth': 1, 'min_samples_split': 110},\n",
       "  {'max_depth': 1, 'min_samples_split': 130},\n",
       "  {'max_depth': 1, 'min_samples_split': 150},\n",
       "  {'max_depth': 1, 'min_samples_split': 170},\n",
       "  {'max_depth': 1, 'min_samples_split': 190},\n",
       "  {'max_depth': 1, 'min_samples_split': 210},\n",
       "  {'max_depth': 1, 'min_samples_split': 230},\n",
       "  {'max_depth': 1, 'min_samples_split': 250},\n",
       "  {'max_depth': 1, 'min_samples_split': 270},\n",
       "  {'max_depth': 1, 'min_samples_split': 290},\n",
       "  {'max_depth': 1, 'min_samples_split': 310},\n",
       "  {'max_depth': 1, 'min_samples_split': 330},\n",
       "  {'max_depth': 1, 'min_samples_split': 350},\n",
       "  {'max_depth': 1, 'min_samples_split': 370},\n",
       "  {'max_depth': 1, 'min_samples_split': 390},\n",
       "  {'max_depth': 1, 'min_samples_split': 410},\n",
       "  {'max_depth': 1, 'min_samples_split': 430},\n",
       "  {'max_depth': 1, 'min_samples_split': 450},\n",
       "  {'max_depth': 1, 'min_samples_split': 470},\n",
       "  {'max_depth': 1, 'min_samples_split': 490},\n",
       "  {'max_depth': 2, 'min_samples_split': 10},\n",
       "  {'max_depth': 2, 'min_samples_split': 30},\n",
       "  {'max_depth': 2, 'min_samples_split': 50},\n",
       "  {'max_depth': 2, 'min_samples_split': 70},\n",
       "  {'max_depth': 2, 'min_samples_split': 90},\n",
       "  {'max_depth': 2, 'min_samples_split': 110},\n",
       "  {'max_depth': 2, 'min_samples_split': 130},\n",
       "  {'max_depth': 2, 'min_samples_split': 150},\n",
       "  {'max_depth': 2, 'min_samples_split': 170},\n",
       "  {'max_depth': 2, 'min_samples_split': 190},\n",
       "  {'max_depth': 2, 'min_samples_split': 210},\n",
       "  {'max_depth': 2, 'min_samples_split': 230},\n",
       "  {'max_depth': 2, 'min_samples_split': 250},\n",
       "  {'max_depth': 2, 'min_samples_split': 270},\n",
       "  {'max_depth': 2, 'min_samples_split': 290},\n",
       "  {'max_depth': 2, 'min_samples_split': 310},\n",
       "  {'max_depth': 2, 'min_samples_split': 330},\n",
       "  {'max_depth': 2, 'min_samples_split': 350},\n",
       "  {'max_depth': 2, 'min_samples_split': 370},\n",
       "  {'max_depth': 2, 'min_samples_split': 390},\n",
       "  {'max_depth': 2, 'min_samples_split': 410},\n",
       "  {'max_depth': 2, 'min_samples_split': 430},\n",
       "  {'max_depth': 2, 'min_samples_split': 450},\n",
       "  {'max_depth': 2, 'min_samples_split': 470},\n",
       "  {'max_depth': 2, 'min_samples_split': 490},\n",
       "  {'max_depth': 3, 'min_samples_split': 10},\n",
       "  {'max_depth': 3, 'min_samples_split': 30},\n",
       "  {'max_depth': 3, 'min_samples_split': 50},\n",
       "  {'max_depth': 3, 'min_samples_split': 70},\n",
       "  {'max_depth': 3, 'min_samples_split': 90},\n",
       "  {'max_depth': 3, 'min_samples_split': 110},\n",
       "  {'max_depth': 3, 'min_samples_split': 130},\n",
       "  {'max_depth': 3, 'min_samples_split': 150},\n",
       "  {'max_depth': 3, 'min_samples_split': 170},\n",
       "  {'max_depth': 3, 'min_samples_split': 190},\n",
       "  {'max_depth': 3, 'min_samples_split': 210},\n",
       "  {'max_depth': 3, 'min_samples_split': 230},\n",
       "  {'max_depth': 3, 'min_samples_split': 250},\n",
       "  {'max_depth': 3, 'min_samples_split': 270},\n",
       "  {'max_depth': 3, 'min_samples_split': 290},\n",
       "  {'max_depth': 3, 'min_samples_split': 310},\n",
       "  {'max_depth': 3, 'min_samples_split': 330},\n",
       "  {'max_depth': 3, 'min_samples_split': 350},\n",
       "  {'max_depth': 3, 'min_samples_split': 370},\n",
       "  {'max_depth': 3, 'min_samples_split': 390},\n",
       "  {'max_depth': 3, 'min_samples_split': 410},\n",
       "  {'max_depth': 3, 'min_samples_split': 430},\n",
       "  {'max_depth': 3, 'min_samples_split': 450},\n",
       "  {'max_depth': 3, 'min_samples_split': 470},\n",
       "  {'max_depth': 3, 'min_samples_split': 490},\n",
       "  {'max_depth': 4, 'min_samples_split': 10},\n",
       "  {'max_depth': 4, 'min_samples_split': 30},\n",
       "  {'max_depth': 4, 'min_samples_split': 50},\n",
       "  {'max_depth': 4, 'min_samples_split': 70},\n",
       "  {'max_depth': 4, 'min_samples_split': 90},\n",
       "  {'max_depth': 4, 'min_samples_split': 110},\n",
       "  {'max_depth': 4, 'min_samples_split': 130},\n",
       "  {'max_depth': 4, 'min_samples_split': 150},\n",
       "  {'max_depth': 4, 'min_samples_split': 170},\n",
       "  {'max_depth': 4, 'min_samples_split': 190},\n",
       "  {'max_depth': 4, 'min_samples_split': 210},\n",
       "  {'max_depth': 4, 'min_samples_split': 230},\n",
       "  {'max_depth': 4, 'min_samples_split': 250},\n",
       "  {'max_depth': 4, 'min_samples_split': 270},\n",
       "  {'max_depth': 4, 'min_samples_split': 290},\n",
       "  {'max_depth': 4, 'min_samples_split': 310},\n",
       "  {'max_depth': 4, 'min_samples_split': 330},\n",
       "  {'max_depth': 4, 'min_samples_split': 350},\n",
       "  {'max_depth': 4, 'min_samples_split': 370},\n",
       "  {'max_depth': 4, 'min_samples_split': 390},\n",
       "  {'max_depth': 4, 'min_samples_split': 410},\n",
       "  {'max_depth': 4, 'min_samples_split': 430},\n",
       "  {'max_depth': 4, 'min_samples_split': 450},\n",
       "  {'max_depth': 4, 'min_samples_split': 470},\n",
       "  {'max_depth': 4, 'min_samples_split': 490},\n",
       "  {'max_depth': 5, 'min_samples_split': 10},\n",
       "  {'max_depth': 5, 'min_samples_split': 30},\n",
       "  {'max_depth': 5, 'min_samples_split': 50},\n",
       "  {'max_depth': 5, 'min_samples_split': 70},\n",
       "  {'max_depth': 5, 'min_samples_split': 90},\n",
       "  {'max_depth': 5, 'min_samples_split': 110},\n",
       "  {'max_depth': 5, 'min_samples_split': 130},\n",
       "  {'max_depth': 5, 'min_samples_split': 150},\n",
       "  {'max_depth': 5, 'min_samples_split': 170},\n",
       "  {'max_depth': 5, 'min_samples_split': 190},\n",
       "  {'max_depth': 5, 'min_samples_split': 210},\n",
       "  {'max_depth': 5, 'min_samples_split': 230},\n",
       "  {'max_depth': 5, 'min_samples_split': 250},\n",
       "  {'max_depth': 5, 'min_samples_split': 270},\n",
       "  {'max_depth': 5, 'min_samples_split': 290},\n",
       "  {'max_depth': 5, 'min_samples_split': 310},\n",
       "  {'max_depth': 5, 'min_samples_split': 330},\n",
       "  {'max_depth': 5, 'min_samples_split': 350},\n",
       "  {'max_depth': 5, 'min_samples_split': 370},\n",
       "  {'max_depth': 5, 'min_samples_split': 390},\n",
       "  {'max_depth': 5, 'min_samples_split': 410},\n",
       "  {'max_depth': 5, 'min_samples_split': 430},\n",
       "  {'max_depth': 5, 'min_samples_split': 450},\n",
       "  {'max_depth': 5, 'min_samples_split': 470},\n",
       "  {'max_depth': 5, 'min_samples_split': 490},\n",
       "  {'max_depth': 6, 'min_samples_split': 10},\n",
       "  {'max_depth': 6, 'min_samples_split': 30},\n",
       "  {'max_depth': 6, 'min_samples_split': 50},\n",
       "  {'max_depth': 6, 'min_samples_split': 70},\n",
       "  {'max_depth': 6, 'min_samples_split': 90},\n",
       "  {'max_depth': 6, 'min_samples_split': 110},\n",
       "  {'max_depth': 6, 'min_samples_split': 130},\n",
       "  {'max_depth': 6, 'min_samples_split': 150},\n",
       "  {'max_depth': 6, 'min_samples_split': 170},\n",
       "  {'max_depth': 6, 'min_samples_split': 190},\n",
       "  {'max_depth': 6, 'min_samples_split': 210},\n",
       "  {'max_depth': 6, 'min_samples_split': 230},\n",
       "  {'max_depth': 6, 'min_samples_split': 250},\n",
       "  {'max_depth': 6, 'min_samples_split': 270},\n",
       "  {'max_depth': 6, 'min_samples_split': 290},\n",
       "  {'max_depth': 6, 'min_samples_split': 310},\n",
       "  {'max_depth': 6, 'min_samples_split': 330},\n",
       "  {'max_depth': 6, 'min_samples_split': 350},\n",
       "  {'max_depth': 6, 'min_samples_split': 370},\n",
       "  {'max_depth': 6, 'min_samples_split': 390},\n",
       "  {'max_depth': 6, 'min_samples_split': 410},\n",
       "  {'max_depth': 6, 'min_samples_split': 430},\n",
       "  {'max_depth': 6, 'min_samples_split': 450},\n",
       "  {'max_depth': 6, 'min_samples_split': 470},\n",
       "  {'max_depth': 6, 'min_samples_split': 490},\n",
       "  {'max_depth': 7, 'min_samples_split': 10},\n",
       "  {'max_depth': 7, 'min_samples_split': 30},\n",
       "  {'max_depth': 7, 'min_samples_split': 50},\n",
       "  {'max_depth': 7, 'min_samples_split': 70},\n",
       "  {'max_depth': 7, 'min_samples_split': 90},\n",
       "  {'max_depth': 7, 'min_samples_split': 110},\n",
       "  {'max_depth': 7, 'min_samples_split': 130},\n",
       "  {'max_depth': 7, 'min_samples_split': 150},\n",
       "  {'max_depth': 7, 'min_samples_split': 170},\n",
       "  {'max_depth': 7, 'min_samples_split': 190},\n",
       "  {'max_depth': 7, 'min_samples_split': 210},\n",
       "  {'max_depth': 7, 'min_samples_split': 230},\n",
       "  {'max_depth': 7, 'min_samples_split': 250},\n",
       "  {'max_depth': 7, 'min_samples_split': 270},\n",
       "  {'max_depth': 7, 'min_samples_split': 290},\n",
       "  {'max_depth': 7, 'min_samples_split': 310},\n",
       "  {'max_depth': 7, 'min_samples_split': 330},\n",
       "  {'max_depth': 7, 'min_samples_split': 350},\n",
       "  {'max_depth': 7, 'min_samples_split': 370},\n",
       "  {'max_depth': 7, 'min_samples_split': 390},\n",
       "  {'max_depth': 7, 'min_samples_split': 410},\n",
       "  {'max_depth': 7, 'min_samples_split': 430},\n",
       "  {'max_depth': 7, 'min_samples_split': 450},\n",
       "  {'max_depth': 7, 'min_samples_split': 470},\n",
       "  {'max_depth': 7, 'min_samples_split': 490},\n",
       "  {'max_depth': 8, 'min_samples_split': 10},\n",
       "  {'max_depth': 8, 'min_samples_split': 30},\n",
       "  {'max_depth': 8, 'min_samples_split': 50},\n",
       "  {'max_depth': 8, 'min_samples_split': 70},\n",
       "  {'max_depth': 8, 'min_samples_split': 90},\n",
       "  {'max_depth': 8, 'min_samples_split': 110},\n",
       "  {'max_depth': 8, 'min_samples_split': 130},\n",
       "  {'max_depth': 8, 'min_samples_split': 150},\n",
       "  {'max_depth': 8, 'min_samples_split': 170},\n",
       "  {'max_depth': 8, 'min_samples_split': 190},\n",
       "  {'max_depth': 8, 'min_samples_split': 210},\n",
       "  {'max_depth': 8, 'min_samples_split': 230},\n",
       "  {'max_depth': 8, 'min_samples_split': 250},\n",
       "  {'max_depth': 8, 'min_samples_split': 270},\n",
       "  {'max_depth': 8, 'min_samples_split': 290},\n",
       "  {'max_depth': 8, 'min_samples_split': 310},\n",
       "  {'max_depth': 8, 'min_samples_split': 330},\n",
       "  {'max_depth': 8, 'min_samples_split': 350},\n",
       "  {'max_depth': 8, 'min_samples_split': 370},\n",
       "  {'max_depth': 8, 'min_samples_split': 390},\n",
       "  {'max_depth': 8, 'min_samples_split': 410},\n",
       "  {'max_depth': 8, 'min_samples_split': 430},\n",
       "  {'max_depth': 8, 'min_samples_split': 450},\n",
       "  {'max_depth': 8, 'min_samples_split': 470},\n",
       "  {'max_depth': 8, 'min_samples_split': 490},\n",
       "  {'max_depth': 9, 'min_samples_split': 10},\n",
       "  {'max_depth': 9, 'min_samples_split': 30},\n",
       "  {'max_depth': 9, 'min_samples_split': 50},\n",
       "  {'max_depth': 9, 'min_samples_split': 70},\n",
       "  {'max_depth': 9, 'min_samples_split': 90},\n",
       "  {'max_depth': 9, 'min_samples_split': 110},\n",
       "  {'max_depth': 9, 'min_samples_split': 130},\n",
       "  {'max_depth': 9, 'min_samples_split': 150},\n",
       "  {'max_depth': 9, 'min_samples_split': 170},\n",
       "  {'max_depth': 9, 'min_samples_split': 190},\n",
       "  {'max_depth': 9, 'min_samples_split': 210},\n",
       "  {'max_depth': 9, 'min_samples_split': 230},\n",
       "  {'max_depth': 9, 'min_samples_split': 250},\n",
       "  {'max_depth': 9, 'min_samples_split': 270},\n",
       "  {'max_depth': 9, 'min_samples_split': 290},\n",
       "  {'max_depth': 9, 'min_samples_split': 310},\n",
       "  {'max_depth': 9, 'min_samples_split': 330},\n",
       "  {'max_depth': 9, 'min_samples_split': 350},\n",
       "  {'max_depth': 9, 'min_samples_split': 370},\n",
       "  {'max_depth': 9, 'min_samples_split': 390},\n",
       "  {'max_depth': 9, 'min_samples_split': 410},\n",
       "  {'max_depth': 9, 'min_samples_split': 430},\n",
       "  {'max_depth': 9, 'min_samples_split': 450},\n",
       "  {'max_depth': 9, 'min_samples_split': 470},\n",
       "  {'max_depth': 9, 'min_samples_split': 490},\n",
       "  {'max_depth': 10, 'min_samples_split': 10},\n",
       "  {'max_depth': 10, 'min_samples_split': 30},\n",
       "  {'max_depth': 10, 'min_samples_split': 50},\n",
       "  {'max_depth': 10, 'min_samples_split': 70},\n",
       "  {'max_depth': 10, 'min_samples_split': 90},\n",
       "  {'max_depth': 10, 'min_samples_split': 110},\n",
       "  {'max_depth': 10, 'min_samples_split': 130},\n",
       "  {'max_depth': 10, 'min_samples_split': 150},\n",
       "  {'max_depth': 10, 'min_samples_split': 170},\n",
       "  {'max_depth': 10, 'min_samples_split': 190},\n",
       "  {'max_depth': 10, 'min_samples_split': 210},\n",
       "  {'max_depth': 10, 'min_samples_split': 230},\n",
       "  {'max_depth': 10, 'min_samples_split': 250},\n",
       "  {'max_depth': 10, 'min_samples_split': 270},\n",
       "  {'max_depth': 10, 'min_samples_split': 290},\n",
       "  {'max_depth': 10, 'min_samples_split': 310},\n",
       "  {'max_depth': 10, 'min_samples_split': 330},\n",
       "  {'max_depth': 10, 'min_samples_split': 350},\n",
       "  {'max_depth': 10, 'min_samples_split': 370},\n",
       "  {'max_depth': 10, 'min_samples_split': 390},\n",
       "  {'max_depth': 10, 'min_samples_split': 410},\n",
       "  {'max_depth': 10, 'min_samples_split': 430},\n",
       "  {'max_depth': 10, 'min_samples_split': 450},\n",
       "  {'max_depth': 10, 'min_samples_split': 470},\n",
       "  {'max_depth': 10, 'min_samples_split': 490},\n",
       "  {'max_depth': 11, 'min_samples_split': 10},\n",
       "  {'max_depth': 11, 'min_samples_split': 30},\n",
       "  {'max_depth': 11, 'min_samples_split': 50},\n",
       "  {'max_depth': 11, 'min_samples_split': 70},\n",
       "  {'max_depth': 11, 'min_samples_split': 90},\n",
       "  {'max_depth': 11, 'min_samples_split': 110},\n",
       "  {'max_depth': 11, 'min_samples_split': 130},\n",
       "  {'max_depth': 11, 'min_samples_split': 150},\n",
       "  {'max_depth': 11, 'min_samples_split': 170},\n",
       "  {'max_depth': 11, 'min_samples_split': 190},\n",
       "  {'max_depth': 11, 'min_samples_split': 210},\n",
       "  {'max_depth': 11, 'min_samples_split': 230},\n",
       "  {'max_depth': 11, 'min_samples_split': 250},\n",
       "  {'max_depth': 11, 'min_samples_split': 270},\n",
       "  {'max_depth': 11, 'min_samples_split': 290},\n",
       "  {'max_depth': 11, 'min_samples_split': 310},\n",
       "  {'max_depth': 11, 'min_samples_split': 330},\n",
       "  {'max_depth': 11, 'min_samples_split': 350},\n",
       "  {'max_depth': 11, 'min_samples_split': 370},\n",
       "  {'max_depth': 11, 'min_samples_split': 390},\n",
       "  {'max_depth': 11, 'min_samples_split': 410},\n",
       "  {'max_depth': 11, 'min_samples_split': 430},\n",
       "  {'max_depth': 11, 'min_samples_split': 450},\n",
       "  {'max_depth': 11, 'min_samples_split': 470},\n",
       "  {'max_depth': 11, 'min_samples_split': 490},\n",
       "  {'max_depth': 12, 'min_samples_split': 10},\n",
       "  {'max_depth': 12, 'min_samples_split': 30},\n",
       "  {'max_depth': 12, 'min_samples_split': 50},\n",
       "  {'max_depth': 12, 'min_samples_split': 70},\n",
       "  {'max_depth': 12, 'min_samples_split': 90},\n",
       "  {'max_depth': 12, 'min_samples_split': 110},\n",
       "  {'max_depth': 12, 'min_samples_split': 130},\n",
       "  {'max_depth': 12, 'min_samples_split': 150},\n",
       "  {'max_depth': 12, 'min_samples_split': 170},\n",
       "  {'max_depth': 12, 'min_samples_split': 190},\n",
       "  {'max_depth': 12, 'min_samples_split': 210},\n",
       "  {'max_depth': 12, 'min_samples_split': 230},\n",
       "  {'max_depth': 12, 'min_samples_split': 250},\n",
       "  {'max_depth': 12, 'min_samples_split': 270},\n",
       "  {'max_depth': 12, 'min_samples_split': 290},\n",
       "  {'max_depth': 12, 'min_samples_split': 310},\n",
       "  {'max_depth': 12, 'min_samples_split': 330},\n",
       "  {'max_depth': 12, 'min_samples_split': 350},\n",
       "  {'max_depth': 12, 'min_samples_split': 370},\n",
       "  {'max_depth': 12, 'min_samples_split': 390},\n",
       "  {'max_depth': 12, 'min_samples_split': 410},\n",
       "  {'max_depth': 12, 'min_samples_split': 430},\n",
       "  {'max_depth': 12, 'min_samples_split': 450},\n",
       "  {'max_depth': 12, 'min_samples_split': 470},\n",
       "  {'max_depth': 12, 'min_samples_split': 490},\n",
       "  {'max_depth': 13, 'min_samples_split': 10},\n",
       "  {'max_depth': 13, 'min_samples_split': 30},\n",
       "  {'max_depth': 13, 'min_samples_split': 50},\n",
       "  {'max_depth': 13, 'min_samples_split': 70},\n",
       "  {'max_depth': 13, 'min_samples_split': 90},\n",
       "  {'max_depth': 13, 'min_samples_split': 110},\n",
       "  {'max_depth': 13, 'min_samples_split': 130},\n",
       "  {'max_depth': 13, 'min_samples_split': 150},\n",
       "  {'max_depth': 13, 'min_samples_split': 170},\n",
       "  {'max_depth': 13, 'min_samples_split': 190},\n",
       "  {'max_depth': 13, 'min_samples_split': 210},\n",
       "  {'max_depth': 13, 'min_samples_split': 230},\n",
       "  {'max_depth': 13, 'min_samples_split': 250},\n",
       "  {'max_depth': 13, 'min_samples_split': 270},\n",
       "  {'max_depth': 13, 'min_samples_split': 290},\n",
       "  {'max_depth': 13, 'min_samples_split': 310},\n",
       "  {'max_depth': 13, 'min_samples_split': 330},\n",
       "  {'max_depth': 13, 'min_samples_split': 350},\n",
       "  {'max_depth': 13, 'min_samples_split': 370},\n",
       "  {'max_depth': 13, 'min_samples_split': 390},\n",
       "  {'max_depth': 13, 'min_samples_split': 410},\n",
       "  {'max_depth': 13, 'min_samples_split': 430},\n",
       "  {'max_depth': 13, 'min_samples_split': 450},\n",
       "  {'max_depth': 13, 'min_samples_split': 470},\n",
       "  {'max_depth': 13, 'min_samples_split': 490},\n",
       "  {'max_depth': 14, 'min_samples_split': 10},\n",
       "  {'max_depth': 14, 'min_samples_split': 30},\n",
       "  {'max_depth': 14, 'min_samples_split': 50},\n",
       "  {'max_depth': 14, 'min_samples_split': 70},\n",
       "  {'max_depth': 14, 'min_samples_split': 90},\n",
       "  {'max_depth': 14, 'min_samples_split': 110},\n",
       "  {'max_depth': 14, 'min_samples_split': 130},\n",
       "  {'max_depth': 14, 'min_samples_split': 150},\n",
       "  {'max_depth': 14, 'min_samples_split': 170},\n",
       "  {'max_depth': 14, 'min_samples_split': 190},\n",
       "  {'max_depth': 14, 'min_samples_split': 210},\n",
       "  {'max_depth': 14, 'min_samples_split': 230},\n",
       "  {'max_depth': 14, 'min_samples_split': 250},\n",
       "  {'max_depth': 14, 'min_samples_split': 270},\n",
       "  {'max_depth': 14, 'min_samples_split': 290},\n",
       "  {'max_depth': 14, 'min_samples_split': 310},\n",
       "  {'max_depth': 14, 'min_samples_split': 330},\n",
       "  {'max_depth': 14, 'min_samples_split': 350},\n",
       "  {'max_depth': 14, 'min_samples_split': 370},\n",
       "  {'max_depth': 14, 'min_samples_split': 390},\n",
       "  {'max_depth': 14, 'min_samples_split': 410},\n",
       "  {'max_depth': 14, 'min_samples_split': 430},\n",
       "  {'max_depth': 14, 'min_samples_split': 450},\n",
       "  {'max_depth': 14, 'min_samples_split': 470},\n",
       "  {'max_depth': 14, 'min_samples_split': 490},\n",
       "  {'max_depth': 15, 'min_samples_split': 10},\n",
       "  {'max_depth': 15, 'min_samples_split': 30},\n",
       "  {'max_depth': 15, 'min_samples_split': 50},\n",
       "  {'max_depth': 15, 'min_samples_split': 70},\n",
       "  {'max_depth': 15, 'min_samples_split': 90},\n",
       "  {'max_depth': 15, 'min_samples_split': 110},\n",
       "  {'max_depth': 15, 'min_samples_split': 130},\n",
       "  {'max_depth': 15, 'min_samples_split': 150},\n",
       "  {'max_depth': 15, 'min_samples_split': 170},\n",
       "  {'max_depth': 15, 'min_samples_split': 190},\n",
       "  {'max_depth': 15, 'min_samples_split': 210},\n",
       "  {'max_depth': 15, 'min_samples_split': 230},\n",
       "  {'max_depth': 15, 'min_samples_split': 250},\n",
       "  {'max_depth': 15, 'min_samples_split': 270},\n",
       "  {'max_depth': 15, 'min_samples_split': 290},\n",
       "  {'max_depth': 15, 'min_samples_split': 310},\n",
       "  {'max_depth': 15, 'min_samples_split': 330},\n",
       "  {'max_depth': 15, 'min_samples_split': 350},\n",
       "  {'max_depth': 15, 'min_samples_split': 370},\n",
       "  {'max_depth': 15, 'min_samples_split': 390},\n",
       "  {'max_depth': 15, 'min_samples_split': 410},\n",
       "  {'max_depth': 15, 'min_samples_split': 430},\n",
       "  {'max_depth': 15, 'min_samples_split': 450},\n",
       "  {'max_depth': 15, 'min_samples_split': 470},\n",
       "  {'max_depth': 15, 'min_samples_split': 490},\n",
       "  {'max_depth': 16, 'min_samples_split': 10},\n",
       "  {'max_depth': 16, 'min_samples_split': 30},\n",
       "  {'max_depth': 16, 'min_samples_split': 50},\n",
       "  {'max_depth': 16, 'min_samples_split': 70},\n",
       "  {'max_depth': 16, 'min_samples_split': 90},\n",
       "  {'max_depth': 16, 'min_samples_split': 110},\n",
       "  {'max_depth': 16, 'min_samples_split': 130},\n",
       "  {'max_depth': 16, 'min_samples_split': 150},\n",
       "  {'max_depth': 16, 'min_samples_split': 170},\n",
       "  {'max_depth': 16, 'min_samples_split': 190},\n",
       "  {'max_depth': 16, 'min_samples_split': 210},\n",
       "  {'max_depth': 16, 'min_samples_split': 230},\n",
       "  {'max_depth': 16, 'min_samples_split': 250},\n",
       "  {'max_depth': 16, 'min_samples_split': 270},\n",
       "  {'max_depth': 16, 'min_samples_split': 290},\n",
       "  {'max_depth': 16, 'min_samples_split': 310},\n",
       "  {'max_depth': 16, 'min_samples_split': 330},\n",
       "  {'max_depth': 16, 'min_samples_split': 350},\n",
       "  {'max_depth': 16, 'min_samples_split': 370},\n",
       "  {'max_depth': 16, 'min_samples_split': 390},\n",
       "  {'max_depth': 16, 'min_samples_split': 410},\n",
       "  {'max_depth': 16, 'min_samples_split': 430},\n",
       "  {'max_depth': 16, 'min_samples_split': 450},\n",
       "  {'max_depth': 16, 'min_samples_split': 470},\n",
       "  {'max_depth': 16, 'min_samples_split': 490},\n",
       "  {'max_depth': 17, 'min_samples_split': 10},\n",
       "  {'max_depth': 17, 'min_samples_split': 30},\n",
       "  {'max_depth': 17, 'min_samples_split': 50},\n",
       "  {'max_depth': 17, 'min_samples_split': 70},\n",
       "  {'max_depth': 17, 'min_samples_split': 90},\n",
       "  {'max_depth': 17, 'min_samples_split': 110},\n",
       "  {'max_depth': 17, 'min_samples_split': 130},\n",
       "  {'max_depth': 17, 'min_samples_split': 150},\n",
       "  {'max_depth': 17, 'min_samples_split': 170},\n",
       "  {'max_depth': 17, 'min_samples_split': 190},\n",
       "  {'max_depth': 17, 'min_samples_split': 210},\n",
       "  {'max_depth': 17, 'min_samples_split': 230},\n",
       "  {'max_depth': 17, 'min_samples_split': 250},\n",
       "  {'max_depth': 17, 'min_samples_split': 270},\n",
       "  {'max_depth': 17, 'min_samples_split': 290},\n",
       "  {'max_depth': 17, 'min_samples_split': 310},\n",
       "  {'max_depth': 17, 'min_samples_split': 330},\n",
       "  {'max_depth': 17, 'min_samples_split': 350},\n",
       "  {'max_depth': 17, 'min_samples_split': 370},\n",
       "  {'max_depth': 17, 'min_samples_split': 390},\n",
       "  {'max_depth': 17, 'min_samples_split': 410},\n",
       "  {'max_depth': 17, 'min_samples_split': 430},\n",
       "  {'max_depth': 17, 'min_samples_split': 450},\n",
       "  {'max_depth': 17, 'min_samples_split': 470},\n",
       "  {'max_depth': 17, 'min_samples_split': 490}],\n",
       " 'split0_test_score': array([0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.80291971, 0.80291971, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.80291971, 0.80291971, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.73722628, 0.74452555, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.72992701, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.73722628, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.73722628, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.72262774, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.69343066, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.69343066, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.69343066, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.69343066, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.69343066, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.69343066, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.69343066, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.69343066, 0.73722628, 0.76642336, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.81751825,\n",
       "        0.81751825, 0.81751825, 0.81751825, 0.81751825, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ]),\n",
       " 'split1_test_score': array([0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.78832117, 0.78832117, 0.78832117, 0.78832117, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.78832117, 0.78832117, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.78832117, 0.78832117, 0.78832117, 0.78832117, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.78832117, 0.78832117, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.72262774, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.75912409, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.75182482, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.74452555, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.72262774, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.72262774, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.70072993, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.70072993, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.70072993, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.70072993, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.70072993, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.70072993, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.70072993, 0.72262774, 0.76642336, 0.76642336, 0.76642336,\n",
       "        0.76642336, 0.76642336, 0.76642336, 0.76642336, 0.78832117,\n",
       "        0.78832117, 0.78832117, 0.80291971, 0.80291971, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ]),\n",
       " 'split2_test_score': array([0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.83211679, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.83211679, 0.82481752, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.83941606, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.76642336, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.77372263, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.76642336, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.76642336, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.75912409, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.75182482, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.75182482, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.74452555, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.74452555, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.74452555, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.74452555, 0.83211679, 0.83211679, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.82481752,\n",
       "        0.82481752, 0.82481752, 0.82481752, 0.82481752, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ]),\n",
       " 'mean_test_score': array([0.81508516, 0.81508516, 0.81508516, 0.81508516, 0.81508516,\n",
       "        0.81508516, 0.81508516, 0.81508516, 0.81508516, 0.81508516,\n",
       "        0.81508516, 0.81508516, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.8053528 , 0.8053528 , 0.81021898, 0.81021898, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81021898, 0.81021898, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.8053528 , 0.8053528 , 0.81021898, 0.81021898, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81021898, 0.81021898, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.77858881, 0.77858881, 0.78588808, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.76155718, 0.76155718, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.77858881, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.75182482, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.74695864, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7323601 , 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.72749392, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.71776156, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.71532847, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.71532847, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.71289538, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.71289538, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.71289538, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.71289538, 0.76399027, 0.78832117, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.80291971, 0.80291971, 0.80291971, 0.81021898,\n",
       "        0.81021898, 0.81021898, 0.81508516, 0.81508516, 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ,\n",
       "        0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 , 0.7080292 ]),\n",
       " 'std_test_score': array([0.00910379, 0.00910379, 0.00910379, 0.00910379, 0.00910379,\n",
       "        0.00910379, 0.00910379, 0.00910379, 0.00910379, 0.00910379,\n",
       "        0.00910379, 0.00910379, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01499857, 0.01499857, 0.01576823, 0.01576823, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.01576823, 0.01576823, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01499857, 0.01499857, 0.01576823, 0.01576823, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.01576823, 0.01576823, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0396825 , 0.03388902, 0.02752727, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04998209, 0.0451271 , 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04393058, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01191966, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02093023, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02481275, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02999715, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02939914, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02597829, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02597829, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02256355, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02256355, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02256355, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02256355, 0.04853999, 0.03096818, 0.02597829, 0.02597829,\n",
       "        0.02597829, 0.02597829, 0.02597829, 0.02597829, 0.01576823,\n",
       "        0.01576823, 0.01576823, 0.00910379, 0.00910379, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]),\n",
       " 'rank_test_score': array([  1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 109,\n",
       "        109,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,   1,   1,\n",
       "        239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 109, 109,\n",
       "         47,  47,  47,  47,  47,  47,  47,  47,  47,  47,   1,   1, 239,\n",
       "        239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 211, 211, 210,\n",
       "        113, 113, 113, 113, 113, 113,  47,  47,  47,   1,   1, 239, 239,\n",
       "        239, 239, 239, 239, 239, 239, 239, 239, 239, 226, 226, 197, 113,\n",
       "        113, 113, 113, 113, 113,  47,  47,  47,   1,   1, 239, 239, 239,\n",
       "        239, 239, 239, 239, 239, 239, 239, 239, 211, 214, 197, 113, 113,\n",
       "        113, 113, 113, 113,  47,  47,  47,   1,   1, 239, 239, 239, 239,\n",
       "        239, 239, 239, 239, 239, 239, 239, 228, 214, 197, 113, 113, 113,\n",
       "        113, 113, 113,  47,  47,  47,   1,   1, 239, 239, 239, 239, 239,\n",
       "        239, 239, 239, 239, 239, 239, 229, 214, 197, 113, 113, 113, 113,\n",
       "        113, 113,  47,  47,  47,   1,   1, 239, 239, 239, 239, 239, 239,\n",
       "        239, 239, 239, 239, 239, 230, 214, 197, 113, 113, 113, 113, 113,\n",
       "        113,  47,  47,  47,   1,   1, 239, 239, 239, 239, 239, 239, 239,\n",
       "        239, 239, 239, 239, 231, 214, 197, 113, 113, 113, 113, 113, 113,\n",
       "         47,  47,  47,   1,   1, 239, 239, 239, 239, 239, 239, 239, 239,\n",
       "        239, 239, 239, 232, 214, 197, 113, 113, 113, 113, 113, 113,  47,\n",
       "         47,  47,   1,   1, 239, 239, 239, 239, 239, 239, 239, 239, 239,\n",
       "        239, 239, 233, 214, 197, 113, 113, 113, 113, 113, 113,  47,  47,\n",
       "         47,   1,   1, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,\n",
       "        239, 233, 214, 197, 113, 113, 113, 113, 113, 113,  47,  47,  47,\n",
       "          1,   1, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,\n",
       "        235, 214, 197, 113, 113, 113, 113, 113, 113,  47,  47,  47,   1,\n",
       "          1, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 235,\n",
       "        214, 197, 113, 113, 113, 113, 113, 113,  47,  47,  47,   1,   1,\n",
       "        239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 235, 214,\n",
       "        197, 113, 113, 113, 113, 113, 113,  47,  47,  47,   1,   1, 239,\n",
       "        239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 235, 214, 197,\n",
       "        113, 113, 113, 113, 113, 113,  47,  47,  47,   1,   1, 239, 239,\n",
       "        239, 239, 239, 239, 239, 239, 239, 239, 239])}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 1, 'min_samples_split': 10}"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc is  0.8150851581508516\n",
      "test acc is  0.7980295566502463\n"
     ]
    }
   ],
   "source": [
    "# clf = DecisionTreeClassifier(random_state=0)\n",
    "# print(clf)\n",
    "# clf=clf.fit(x_train,np.squeeze(y_train.values))\n",
    "# print(clf.feature_importances_,end='\\n\\n\\n')\n",
    "train_preds=clf.predict(x_train)\n",
    "train_acc=accuracy_score(y_train, train_preds)\n",
    "print('train acc is ',train_acc)\n",
    "\n",
    "\n",
    "test_preds=clf.predict(x_test)\n",
    "test_acc=accuracy_score(y_test,test_preds)\n",
    "print('test acc is ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('dt.txt',train_preds,delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is it a joke or what we have same thing with LR and SVMs .If we go with min_samples_split=2 and max_depth =1  than train accuracy is 1 and test acc=71 %**\n",
    "\n",
    "\n",
    "**let us see the most important features, and exculde all the non relevant features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dt=clf.best_estimator_\n",
    "best_dt.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames=['Gender','Married','Dependents', 'Education',\n",
    " 'Self_Employed', 'ApplicantIncome',\n",
    " 'CoapplicantIncome', 'LoanAmount',\n",
    " 'Loan_Amount_Term', 'Credit_History',\n",
    " 'Property_Area',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(167.4, 163.07999999999998, 'Credit_History <= 0.5\\nentropy = 0.413\\nsamples = 411\\nvalue = [120, 291]\\nclass = Yes'),\n",
       " Text(83.7, 54.360000000000014, 'entropy = 0.168\\nsamples = 54\\nvalue = [49, 5]\\nclass = NO'),\n",
       " Text(251.10000000000002, 54.360000000000014, 'entropy = 0.319\\nsamples = 357\\nvalue = [71, 286]\\nclass = Yes')]"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree.plot_tree(best_dt,feature_names=fnames,class_names=['NO','Yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit History seems promising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Work let us use Ensembling\n",
    "\n",
    "\n",
    "If Ensembles also cannot Work Then use NN\n",
    "\n",
    "##### Try Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc is 0.7469586374695864 at depth =1\n",
      "test acc is 0.6896551724137931 at depth =1 \n",
      "\n",
      "train acc is 0.8150851581508516 at depth =2\n",
      "test acc is 0.7980295566502463 at depth =2 \n",
      "\n",
      "train acc is 0.8175182481751825 at depth =3\n",
      "test acc is 0.7980295566502463 at depth =3 \n",
      "\n",
      "train acc is 0.8248175182481752 at depth =4\n",
      "test acc is 0.7980295566502463 at depth =4 \n",
      "\n",
      "train acc is 0.8442822384428224 at depth =5\n",
      "test acc is 0.8029556650246306 at depth =5 \n",
      "\n",
      "train acc is 0.8613138686131386 at depth =6\n",
      "test acc is 0.8078817733990148 at depth =6 \n",
      "\n",
      "train acc is 0.8905109489051095 at depth =7\n",
      "test acc is 0.8029556650246306 at depth =7 \n",
      "\n",
      "train acc is 0.9124087591240876 at depth =8\n",
      "test acc is 0.812807881773399 at depth =8 \n",
      "\n",
      "train acc is 0.9416058394160584 at depth =9\n",
      "test acc is 0.7931034482758621 at depth =9 \n",
      "\n",
      "train acc is 0.9659367396593674 at depth =10\n",
      "test acc is 0.7832512315270936 at depth =10 \n",
      "\n",
      "train acc is 0.9878345498783455 at depth =11\n",
      "test acc is 0.7832512315270936 at depth =11 \n",
      "\n",
      "train acc is 0.9951338199513382 at depth =12\n",
      "test acc is 0.7832512315270936 at depth =12 \n",
      "\n",
      "train acc is 1.0 at depth =13\n",
      "test acc is 0.7783251231527094 at depth =13 \n",
      "\n",
      "train acc is 1.0 at depth =14\n",
      "test acc is 0.7881773399014779 at depth =14 \n",
      "\n",
      "train acc is 1.0 at depth =15\n",
      "test acc is 0.7832512315270936 at depth =15 \n",
      "\n",
      "train acc is 1.0 at depth =16\n",
      "test acc is 0.7733990147783252 at depth =16 \n",
      "\n",
      "train acc is 1.0 at depth =17\n",
      "test acc is 0.7783251231527094 at depth =17 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "for d in range(1,18,1):    \n",
    "    clf_rf = RandomForestClassifier(n_estimators=100, max_depth=d,random_state=0,bootstrap=False) \n",
    "    # max_depth,n_estimators,  to tune\n",
    "    clf_rf.fit(x_train, np.squeeze(y_train.values))  \n",
    "#     print(clf)\n",
    "#     print(clf.feature_importances_)\n",
    "\n",
    "    train_preds=clf_rf.predict(x_train)\n",
    "    train_acc=accuracy_score(y_train, train_preds)\n",
    "    print('train acc is {} at depth ={}'.format(train_acc,d))\n",
    "\n",
    "\n",
    "    test_preds=clf_rf.predict(x_test)\n",
    "    test_acc=accuracy_score(y_test,test_preds)\n",
    "    print('test acc is {} at depth ={} '.format(test_acc,d))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see if we do Grid Search than we get result as above or not\n",
    "\n",
    "**No not getting same results, some issues with GridSearch code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 17 candidates, totalling 51 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  51 | elapsed:    4.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=False,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=1,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False, random_state=0,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1, param_grid={'max_depth': range(1, 18)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(accuracy_score), verbose=2)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=1,random_state=0,bootstrap=False) \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "parameters = {'max_depth':range(1,18)}\n",
    "clf = GridSearchCV(clf_rf, parameters, cv=3,verbose=2,n_jobs=-1,scoring=make_scorer(accuracy_score))\n",
    "clf.fit(x_train, np.squeeze(y_train.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.31446369, 0.24434749, 0.24434741, 0.35076006, 0.36861817,\n",
       "        0.26994522, 0.30378167, 0.27260439, 0.3171277 , 0.39328106,\n",
       "        0.48904403, 0.43284249, 0.31515718, 0.29760806, 0.30091588,\n",
       "        0.3217837 , 0.24736969]),\n",
       " 'std_fit_time': array([0.00263282, 0.02690919, 0.01547243, 0.01181163, 0.02708004,\n",
       "        0.01896315, 0.01680427, 0.00470179, 0.00997288, 0.04278345,\n",
       "        0.03305428, 0.02361527, 0.00293623, 0.00334201, 0.01295701,\n",
       "        0.02313027, 0.01006724]),\n",
       " 'mean_score_time': array([0.02260613, 0.02548464, 0.03492832, 0.05659262, 0.02559813,\n",
       "        0.02958814, 0.02626348, 0.02493366, 0.03332472, 0.04888161,\n",
       "        0.04695264, 0.03490837, 0.02493413, 0.02785452, 0.02986582,\n",
       "        0.02659583, 0.01860134]),\n",
       " 'std_score_time': array([0.00094038, 0.00318374, 0.00565718, 0.00806596, 0.00204914,\n",
       "        0.00384753, 0.00046952, 0.0014113 , 0.01619612, 0.00327062,\n",
       "        0.01059723, 0.00407362, 0.00081459, 0.0008871 , 0.00274421,\n",
       "        0.00285997, 0.00059386]),\n",
       " 'param_max_depth': masked_array(data=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n",
       "                    17],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 1},\n",
       "  {'max_depth': 2},\n",
       "  {'max_depth': 3},\n",
       "  {'max_depth': 4},\n",
       "  {'max_depth': 5},\n",
       "  {'max_depth': 6},\n",
       "  {'max_depth': 7},\n",
       "  {'max_depth': 8},\n",
       "  {'max_depth': 9},\n",
       "  {'max_depth': 10},\n",
       "  {'max_depth': 11},\n",
       "  {'max_depth': 12},\n",
       "  {'max_depth': 13},\n",
       "  {'max_depth': 14},\n",
       "  {'max_depth': 15},\n",
       "  {'max_depth': 16},\n",
       "  {'max_depth': 17}],\n",
       " 'split0_test_score': array([0.7080292 , 0.81751825, 0.81021898, 0.81751825, 0.81751825,\n",
       "        0.81021898, 0.81021898, 0.81021898, 0.80291971, 0.81751825,\n",
       "        0.82481752, 0.81021898, 0.81021898, 0.83211679, 0.82481752,\n",
       "        0.83211679, 0.82481752]),\n",
       " 'split1_test_score': array([0.7810219 , 0.79562044, 0.78832117, 0.78832117, 0.79562044,\n",
       "        0.79562044, 0.78832117, 0.77372263, 0.76642336, 0.76642336,\n",
       "        0.75912409, 0.75182482, 0.75182482, 0.75182482, 0.73722628,\n",
       "        0.73722628, 0.73722628]),\n",
       " 'split2_test_score': array([0.75182482, 0.82481752, 0.82481752, 0.81751825, 0.81751825,\n",
       "        0.81021898, 0.81751825, 0.81021898, 0.80291971, 0.80291971,\n",
       "        0.80291971, 0.79562044, 0.81021898, 0.79562044, 0.80291971,\n",
       "        0.79562044, 0.7810219 ]),\n",
       " 'mean_test_score': array([0.74695864, 0.81265207, 0.80778589, 0.80778589, 0.81021898,\n",
       "        0.8053528 , 0.8053528 , 0.79805353, 0.79075426, 0.79562044,\n",
       "        0.79562044, 0.78588808, 0.79075426, 0.79318735, 0.78832117,\n",
       "        0.78832117, 0.7810219 ]),\n",
       " 'std_test_score': array([0.02999715, 0.01240637, 0.01499857, 0.01376364, 0.01032273,\n",
       "        0.00688182, 0.01240637, 0.01720454, 0.01720454, 0.02148847,\n",
       "        0.02731137, 0.02481275, 0.02752727, 0.03282418, 0.03721912,\n",
       "        0.03908121, 0.03575897]),\n",
       " 'rank_test_score': array([17,  1,  3,  3,  2,  5,  5,  7, 11,  8,  8, 15, 11, 10, 13, 13, 16])}"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2}"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc is 0.8150851581508516\n",
      "test acc is 0.7980295566502463 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_preds=clf.predict(x_train)\n",
    "train_acc=accuracy_score(y_train, train_preds)\n",
    "print('train acc is {}'.format(train_acc))\n",
    "\n",
    "\n",
    "test_preds=clf.predict(x_test)\n",
    "test_acc=accuracy_score(y_test,test_preds)\n",
    "print('test acc is {} '.format(test_acc))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Try Boosting \n",
    "###### Try XG Boost Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm goes by lots of different names such as gradient boosting, multiple additive regression trees, stochastic gradient boosting or gradient boosting machines.\n",
    "\n",
    "Boosting is an ensemble technique where new models are added to correct the errors made by existing models. Models are added sequentially until no further improvements can be made. A popular example is the AdaBoost algorithm that weights data points that are hard to predict.\n",
    "\n",
    "Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.\n",
    "\n",
    "This approach supports both regression and classification predictive modeling problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "train acc is  0.8710462287104623\n",
      "test acc is  0.7980295566502463\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, np.squeeze(y_train.values))\n",
    "print(model)\n",
    "\n",
    "train_preds = model.predict(x_train)\n",
    "train_preds = [round(value) for value in train_preds]\n",
    "# model predict probs from 0 to 1 and thus we round them to app value\n",
    "train_acc=accuracy_score(y_train, train_preds)\n",
    "print('train acc is ',train_acc)\n",
    "\n",
    "\n",
    "test_preds=model.predict(x_test)\n",
    "test_acc=accuracy_score(y_test,test_preds)\n",
    "print('test acc is ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Acc shot up, let us tune this https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us Use NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Day of Judgement has come\n",
    "\n",
    "Let us make predictions on test Data\n",
    "\n",
    "Steps : Preprocess the data and feed it to the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001015</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5720</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001022</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3076</td>\n",
       "      <td>1500</td>\n",
       "      <td>126.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001031</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5000</td>\n",
       "      <td>1800</td>\n",
       "      <td>208.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001035</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2340</td>\n",
       "      <td>2546</td>\n",
       "      <td>100.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001051</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>3276</td>\n",
       "      <td>0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001015   Male     Yes          0      Graduate            No   \n",
       "1  LP001022   Male     Yes          1      Graduate            No   \n",
       "2  LP001031   Male     Yes          2      Graduate            No   \n",
       "3  LP001035   Male     Yes          2      Graduate            No   \n",
       "4  LP001051   Male      No          0  Not Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5720                  0       110.0             360.0   \n",
       "1             3076               1500       126.0             360.0   \n",
       "2             5000               1800       208.0             360.0   \n",
       "3             2340               2546       100.0             360.0   \n",
       "4             3276                  0        78.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area  \n",
       "0             1.0         Urban  \n",
       "1             1.0         Urban  \n",
       "2             1.0         Urban  \n",
       "3             NaN         Urban  \n",
       "4             1.0         Urban  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv('test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loan_ID               0\n",
       "Gender               11\n",
       "Married               0\n",
       "Dependents           10\n",
       "Education             0\n",
       "Self_Employed        23\n",
       "ApplicantIncome       0\n",
       "CoapplicantIncome     0\n",
       "LoanAmount            5\n",
       "Loan_Amount_Term      6\n",
       "Credit_History       29\n",
       "Property_Area         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessTest(train):\n",
    "    global means, scalar\n",
    "# High Nans in gender, married, dependents, self_employed, loanAmount, loanTerms, credit_history\n",
    "\n",
    "    train['Gender']=train['Gender'].fillna('Male')\n",
    "    train['Married']=train['Married'].fillna('Yes')\n",
    "    train['Dependents']=train['Dependents'].fillna('0')\n",
    "    train['Self_Employed']=train['Self_Employed'].fillna('No')\n",
    "    train['Credit_History']=train['Credit_History'].fillna('1')\n",
    "#     filling the nas with mean for numeric real values\n",
    "    train=train.fillna(means)\n",
    "    global list_encoders\n",
    "    global columns\n",
    "#     columns=['Gender','Married','Education','Self_Employed','Property_Area']\n",
    "    for c,enc in zip(columns,list_encoders):\n",
    "#         cEncoder=LabelEncoder()\n",
    "        train[c]=enc.transform(train[c].astype('str'))\n",
    "#         list_encoders.append(cEncoder)\n",
    "    \n",
    "#     coverting 3+ in train to 3\n",
    "    train['Dependents']=train['Dependents'].replace(['3+'], '3')\n",
    "#     train['Loan_Status']=train['Loan_Status'].replace(['Y','N'],[1,0])\n",
    "    # coverting dependents column to integer\n",
    "    train['Dependents']=train['Dependents'].astype(str).astype(int)\n",
    "    \n",
    "#     no one hot encode the columns \n",
    "    col_to_one_hot=['Gender','Married','Education','Self_employed']\n",
    "#     I dont think there is some need to one hot the columns, It would introduce the co linearity in the dataset\n",
    "    \n",
    "    train.drop(['Loan_ID'],axis=1,inplace=True)\n",
    "#     scalar = StandardScaler()\n",
    "    train_big_num=train[['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term']]\n",
    "#     scalar.fit(train_big_num)\n",
    "#     print(scaler.mean_)\n",
    "\n",
    "    train_np=scalar.transform(train_big_num)\n",
    "    \n",
    "    scaled_features=pd.DataFrame(data=train_np,columns=['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term'])\n",
    "    \n",
    "    sc_feat=['ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term']\n",
    "    for sf in sc_feat:\n",
    "        train[sf]=scaled_features[sf]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ppd=preprocessTest(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051857</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.433638</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.381297</td>\n",
       "      <td>-0.041468</td>\n",
       "      <td>-0.243092</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.066097</td>\n",
       "      <td>0.061136</td>\n",
       "      <td>0.733459</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.501872</td>\n",
       "      <td>0.316278</td>\n",
       "      <td>-0.552730</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.348532</td>\n",
       "      <td>-0.554487</td>\n",
       "      <td>-0.814731</td>\n",
       "      <td>0.279851</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Married  Dependents  Education  Self_Employed  ApplicantIncome  \\\n",
       "0       1        1           0          0              0         0.051857   \n",
       "1       1        1           1          0              0        -0.381297   \n",
       "2       1        1           2          0              0        -0.066097   \n",
       "3       1        1           2          0              0        -0.501872   \n",
       "4       1        0           0          1              0        -0.348532   \n",
       "\n",
       "   CoapplicantIncome  LoanAmount  Loan_Amount_Term Credit_History  \\\n",
       "0          -0.554487   -0.433638          0.279851              1   \n",
       "1          -0.041468   -0.243092          0.279851              1   \n",
       "2           0.061136    0.733459          0.279851              1   \n",
       "3           0.316278   -0.552730          0.279851              1   \n",
       "4          -0.554487   -0.814731          0.279851              1   \n",
       "\n",
       "   Property_Area  \n",
       "0              2  \n",
       "1              2  \n",
       "2              2  \n",
       "3              2  \n",
       "4              2  "
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ppd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids=test['Loan_ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Predictions and submit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
      "                   multi_class='warn', n_jobs=1, penalty='l1', random_state=0,\n",
      "                   solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_preds=clf_lr.predict(test_ppd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_series=pd.Series(submit_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = { 'Loan_ID': test_ids, 'Loan_Status': preds_series } \n",
    "  \n",
    "result = pd.DataFrame(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001035</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001051</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID  Loan_Status\n",
       "0  LP001015            1\n",
       "1  LP001022            1\n",
       "2  LP001031            1\n",
       "3  LP001035            1\n",
       "4  LP001051            1"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['Loan_Status']=train['Loan_Status'].replace([1,0],['Y','N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('preds.csv',sep=',',header=True,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
